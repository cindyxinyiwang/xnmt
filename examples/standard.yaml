standard:
  experiment:
    model_file: model/<EXP>.mod
    #hyp_file: model/<EXP>.hyp
    out_file: model/<EXP>.out
    err_file: model/<EXP>.err
    cfg_file: model/<EXP>.yaml
    run_for_epochs: 20
    score_nbest: True
    #eval_metrics: bleu
    eval_only: True
  train: !TrainingRegimen
    pretrained_model_file: model/standard.mod
    dev_every: 2
    batcher: !SrcBatcher
      batch_size: 1
    glob:
      dropout: 0.1
      default_layer_dim: 512
    restart_trainer: True
    trainer: !AdamTrainer
      alpha: 0.0002
    lr_decay: 0.5
    dev_metrics: bleu
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
      training_corpus: !BilingualTrainingCorpus
        train_src: examples/data/head.ja
        train_trg: examples/data/head.en
        dev_src: examples/data/head.ja
        dev_trg: examples/data/head.en
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder {}
      encoder: !BiLSTMSeqTransducer
        layers: 1
      attender: !MlpAttender {}
      trg_embedder: !SimpleWordEmbedder {}
      decoder: !MlpSoftmaxDecoder
        layers: 1
        mlp_hidden_dim: 512
        bridge: !CopyBridge {}
        label_smoothing: 0.1
  decode: !XnmtDecoder
    beam: 5
    len_norm_type: !PolynomialNormalization
      apply_during_search: true
    src_file: examples/data/head.ja
    nbest_file: examples/data/head.en.nbest
    mode: score_nbest
    nbest: 2
  #evaluate:
  #  ref_file: examples/data/head.en