+ set -e
+ module load singularity
++ /usr/bin/modulecmd bash load singularity
+ eval LD_LIBRARY_PATH=/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=rocks-openmpi:singularity' ';export' 'LOADEDMODULES;MANPATH=/opt/singularity/share/man:/usr/share/man' ';export' 'MANPATH;PATH=/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin' ';export' 'PATH;SINGULARITY_SHELL=/bin/bash' ';export' 'SINGULARITY_SHELL;_LMFILES_=/usr/share/Modules/modulefiles/rocks-openmpi:/usr/share/Modules/modulefiles/singularity' ';export' '_LMFILES_;'
++ LD_LIBRARY_PATH=/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=rocks-openmpi:singularity
++ export LOADEDMODULES
++ MANPATH=/opt/singularity/share/man:/usr/share/man
++ export MANPATH
++ PATH=/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin
++ export PATH
++ SINGULARITY_SHELL=/bin/bash
++ export SINGULARITY_SHELL
++ _LMFILES_=/usr/share/Modules/modulefiles/rocks-openmpi:/usr/share/Modules/modulefiles/singularity
++ export _LMFILES_
+ singularity shell /projects/tir1/singularity/tensorflow-ubuntu-16.04.2-lts-nvidia-375.26.img
WARNING: Non existant bind point (directory) in container: '/lib/modules'
Singularity: Invoking an interactive shell within container...

+ module load cuda-8.0 cudnn-8.0-5.1
++ /usr/bin/modulecmd bash load cuda-8.0 cudnn-8.0-5.1
+ eval CPATH=/opt/cudnn-5.1/include ';export' 'CPATH;C_INCLUDE_PATH=/opt/cudnn-5.1/include' ';export' 'C_INCLUDE_PATH;LD_LIBRARY_PATH=/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib' ';export' 'LD_LIBRARY_PATH;LOADEDMODULES=rocks-openmpi:singularity:cuda-8.0:cudnn-8.0-5.1' ';export' 'LOADEDMODULES;PATH=/opt/cuda-8.0/bin:/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin' ';export' 'PATH;_LMFILES_=/usr/share/Modules/modulefiles/rocks-openmpi:/usr/share/Modules/modulefiles/singularity:/usr/share/Modules/modulefiles/cuda-8.0:/usr/share/Modules/modulefiles/cudnn-8.0-5.1' ';export' '_LMFILES_;'
++ CPATH=/opt/cudnn-5.1/include
++ export CPATH
++ C_INCLUDE_PATH=/opt/cudnn-5.1/include
++ export C_INCLUDE_PATH
++ LD_LIBRARY_PATH=/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
++ export LD_LIBRARY_PATH
++ LOADEDMODULES=rocks-openmpi:singularity:cuda-8.0:cudnn-8.0-5.1
++ export LOADEDMODULES
++ PATH=/opt/cuda-8.0/bin:/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin
++ export PATH
++ _LMFILES_=/usr/share/Modules/modulefiles/rocks-openmpi:/usr/share/Modules/modulefiles/singularity:/usr/share/Modules/modulefiles/cuda-8.0:/usr/share/Modules/modulefiles/cudnn-8.0-5.1
++ export _LMFILES_
+ export CUDA_HOME=/projects/tir1/cuda-8.0.27.1
+ CUDA_HOME=/projects/tir1/cuda-8.0.27.1
+ PATH=/home/xinyiw1/bin:/opt/cuda-8.0/bin:/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin
+ PATH=/projects/tir1/cuda-8.0.27.1/bin:/home/xinyiw1/bin:/opt/cuda-8.0/bin:/opt/singularity/bin:/home/xinyiw1/miniconda2/envs/research/bin:/opt/cuda-8.0/bin/:/home/xinyiw1/sentencepiece/bin:/home/xinyiw1/boost_1_65/bin:/home/xinyiw1/autoconf-2.69/bin:/home/xinyiw1/gcc-5.2.0/bin:/home/xinyiw1/miniconda2/bin:/home/xinyiw1/gcc-5.2.0/bin:/opt/openmpi/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/ganglia/bin:/opt/ganglia/sbin:/usr/java/latest/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/xinyiw1/bin
+ export PATH
+ export LD_LIBRARY_PATH=/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ LD_LIBRARY_PATH=/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ export CPATH=/projects/tir1/cuda-8.0.27.1/include:/opt/cudnn-5.1/include
+ CPATH=/projects/tir1/cuda-8.0.27.1/include:/opt/cudnn-5.1/include
+ export LIBRARY_PATH=/projects/tir1/cuda-8.0.27.1/lib64:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ LIBRARY_PATH=/projects/tir1/cuda-8.0.27.1/lib64:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ export LD_LIBRARY_PATH=/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ LD_LIBRARY_PATH=/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ export LD_LIBRARY_PATH=/opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ LD_LIBRARY_PATH=/opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ export CPATH=/opt/cudnn-8.0/include:/projects/tir1/cuda-8.0.27.1/include:/opt/cudnn-5.1/include
+ CPATH=/opt/cudnn-8.0/include:/projects/tir1/cuda-8.0.27.1/include:/opt/cudnn-5.1/include
+ export LIBRARY_PATH=/opt/cudnn-8.0/lib64:/opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ LIBRARY_PATH=/opt/cudnn-8.0/lib64:/opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ echo /opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
/opt/cudnn-8.0/lib64:/home/xinyiw1/software/dynet-base/dynet/build/dynet/:/projects/tir1/cuda-8.0.27.1/lib64:/opt/cudnn-5.1/lib64:/opt/cuda-8.0/lib64:/opt/cuda-8.0/lib:/opt/singularity/lib:/opt/cuda-8.0/lib64/:/home/xinyiw1/gcc-5.2.0/lib64:/home/xinyiw1/boost_1_65_1/lib:/home/xinyiw1/gcc-5.2.0/lib:/opt/openmpi/lib
+ model_name=tree
+ python xnmt/xnmt_run_experiments.py examples/kftt_wordlstm.yaml --dynet-gpu --dynet-mem 10000
[dynet] initializing CUDA
[dynet] Request for 1 specific GPU ...
[dynet] Device Number: 0
[dynet]   Device name: TITAN X (Pascal)
[dynet]   Memory Clock Rate (KHz): 5005000
[dynet]   Memory Bus Width (bits): 384
[dynet]   Peak Memory Bandwidth (GB/s): 480.48
[dynet]   Memory Free (GB): 12.619/12.7816
[dynet]
[dynet] Device(s) selected: 0
[dynet] random seed: 23219944
[dynet] allocating memory: 10000MB
[dynet] memory allocation done.
=> Running kftt-tree-wordlstm-2
   > Preprocessing   
   > Training   
   initialized BilingualTrainingCorpus({'train_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja', 'dev_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.en', 'dev_ref_file': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upper.en', 'dev_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja', 'train_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.en'})   
   initialized PlainTextReader({})   
   initialized TreeReader({})   
   initialized BilingualCorpusParser({'trg_reader': <xnmt.input.TreeReader object at 0x7f47394e79d0>, 'src_reader': <xnmt.input.PlainTextReader object at 0x7f47394e7750>})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   initialized SimpleWordEmbedder({'vocab_size': 48841, 'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>})   
   initialized LSTMEncoder({'layers': 1, 'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>})   
   initialized StandardAttender({'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>})   
   initialized SimpleWordEmbedder({'vocab_size': 55991, 'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>})   
   initialized CopyBridge({'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>, 'dec_layers': 1})   
   initialized TreeDecoder({'layers': 1, 'bridge': <xnmt.decoder.CopyBridge object at 0x7f47386dadd0>, 'vocab_size': 55991, 'mlp_hidden_dim': 512, 'context': <xnmt.model_context.ModelContext object at 0x7f47394e74d0>, 'word_lstm': True})   
   ('Skipping hierarchical construction:', 'TranslatorMLELoss', 'is not a subclass of HierarchicalModel')   
   initialized DefaultTranslator({'src_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7f473948c850>, 'decoder': <xnmt.decoder.TreeDecoder object at 0x7f47386daf90>, 'trg_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7f47386dae50>, 'attender': <xnmt.attender.StandardAttender object at 0x7f47386daad0>, 'encoder': <xnmt.encoder.LSTMEncoder object at 0x7f473948c910>})   
   Start training in minibatch mode...   
   Epoch 0.0190: train_loss/word=6.304 (words=71703, words/sec=334.84, time=0-00:03:34)   
   Epoch 0.0380: train_loss/word=5.870 (words=140588, words/sec=332.72, time=0-00:07:01)   
   Epoch 0.0569: train_loss/word=5.331 (words=209301, words/sec=331.95, time=0-00:10:28)   
   Epoch 0.0759: train_loss/word=4.898 (words=281013, words/sec=333.04, time=0-00:14:03)   
   Epoch 0.0949: train_loss/word=4.582 (words=350118, words/sec=332.44, time=0-00:17:31)   
   Epoch 0.1139: train_loss/word=4.341 (words=418609, words/sec=332.92, time=0-00:20:57)   
   Epoch 0.1328: train_loss/word=4.147 (words=489150, words/sec=333.27, time=0-00:24:28)   
   Epoch 0.1518: train_loss/word=3.984 (words=560782, words/sec=333.19, time=0-00:28:03)   
   Epoch 0.1708: train_loss/word=3.850 (words=631817, words/sec=333.05, time=0-00:31:37)   
   Epoch 0.1898: train_loss/word=3.742 (words=700230, words/sec=332.05, time=0-00:35:03)   
   Epoch 0.2087: train_loss/word=3.647 (words=770442, words/sec=332.55, time=0-00:38:34)   
   Epoch 0.2277: train_loss/word=3.568 (words=839288, words/sec=332.66, time=0-00:42:01)   
   Epoch 0.2467: train_loss/word=3.494 (words=908889, words/sec=332.26, time=0-00:45:30)   
   Epoch 0.2657: train_loss/word=3.431 (words=978202, words/sec=332.74, time=0-00:48:58)   
   Epoch 0.2846: train_loss/word=3.371 (words=1046581, words/sec=332.46, time=0-00:52:24)   
   Epoch 0.3036: train_loss/word=3.318 (words=1116084, words/sec=332.47, time=0-00:55:53)   
   Epoch 0.3226: train_loss/word=3.267 (words=1187592, words/sec=332.94, time=0-00:59:28)   
   Epoch 0.3416: train_loss/word=3.223 (words=1257227, words/sec=332.49, time=0-01:02:57)   
   Epoch 0.3606: train_loss/word=3.182 (words=1326006, words/sec=332.85, time=0-01:06:24)   
   Epoch 0.3795: train_loss/word=3.145 (words=1393638, words/sec=331.97, time=0-01:09:48)   
   Epoch 0.3985: train_loss/word=3.108 (words=1463259, words/sec=332.68, time=0-01:13:17)   
   Epoch 0.4175: train_loss/word=3.075 (words=1533987, words/sec=333.25, time=0-01:16:49)   
   Epoch 0.4365: train_loss/word=3.043 (words=1603306, words/sec=332.80, time=0-01:20:18)   
   Epoch 0.4554: train_loss/word=3.014 (words=1672231, words/sec=331.93, time=0-01:23:45)   
   Epoch 0.4744: train_loss/word=2.986 (words=1743438, words/sec=332.96, time=0-01:27:19)   
   Epoch 0.4934: train_loss/word=2.959 (words=1813690, words/sec=332.88, time=0-01:30:50)   
   Epoch 0.5124: train_loss/word=2.935 (words=1881949, words/sec=332.74, time=0-01:34:15)   
   Epoch 0.5313: train_loss/word=2.912 (words=1948821, words/sec=332.20, time=0-01:37:37)   
   Epoch 0.5503: train_loss/word=2.890 (words=2019079, words/sec=332.82, time=0-01:41:08)   
   Epoch 0.5693: train_loss/word=2.870 (words=2087860, words/sec=332.32, time=0-01:44:35)   
   Epoch 0.5883: train_loss/word=2.849 (words=2158523, words/sec=332.54, time=0-01:48:07)   
   Epoch 0.6072: train_loss/word=2.830 (words=2228055, words/sec=332.18, time=0-01:51:36)   
   Epoch 0.6262: train_loss/word=2.812 (words=2296947, words/sec=332.43, time=0-01:55:04)   
   Epoch 0.6452: train_loss/word=2.795 (words=2366869, words/sec=332.59, time=0-01:58:34)   
   Epoch 0.6642: train_loss/word=2.778 (words=2437506, words/sec=332.49, time=0-02:02:06)   
   Epoch 0.6832: train_loss/word=2.762 (words=2504829, words/sec=332.77, time=0-02:05:29)   
   Epoch 0.7021: train_loss/word=2.746 (words=2574523, words/sec=332.89, time=0-02:08:58)   
   Epoch 0.7211: train_loss/word=2.731 (words=2643711, words/sec=332.53, time=0-02:12:26)   
   Epoch 0.7401: train_loss/word=2.716 (words=2715513, words/sec=332.73, time=0-02:16:02)   
   Epoch 0.7591: train_loss/word=2.701 (words=2787768, words/sec=333.14, time=0-02:19:39)   
   Epoch 0.7780: train_loss/word=2.687 (words=2860425, words/sec=332.94, time=0-02:23:17)   
   Epoch 0.7970: train_loss/word=2.674 (words=2931753, words/sec=333.37, time=0-02:26:51)   
   Epoch 0.8160: train_loss/word=2.662 (words=3002970, words/sec=333.26, time=0-02:30:25)   
   Epoch 0.8350: train_loss/word=2.649 (words=3073080, words/sec=332.89, time=0-02:33:55)   
   Epoch 0.8539: train_loss/word=2.636 (words=3145441, words/sec=333.08, time=0-02:37:33)   
   Epoch 0.8729: train_loss/word=2.625 (words=3215005, words/sec=332.63, time=0-02:41:02)   
   Epoch 0.8919: train_loss/word=2.614 (words=3286296, words/sec=333.08, time=0-02:44:36)   
   Epoch 0.9109: train_loss/word=2.604 (words=3355913, words/sec=333.03, time=0-02:48:05)   
   Epoch 0.9298: train_loss/word=2.594 (words=3424362, words/sec=332.33, time=0-02:51:31)   
   Epoch 0.9488: train_loss/word=2.583 (words=3495584, words/sec=333.30, time=0-02:55:04)   
   Epoch 0.9678: train_loss/word=2.573 (words=3565573, words/sec=332.66, time=0-02:58:35)   
   Epoch 0.9868: train_loss/word=2.563 (words=3634887, words/sec=332.40, time=0-03:02:03)   
   Epoch 1.0000: train_loss/word=2.557 (words=3683901, words/sec=332.70, time=0-03:04:31)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 1.0000 dev [auxiliary] BLEU4: 0.0125300855855, 0.260612/0.032915/0.004777/0.001612 (BP = 0.781561, ratio=0.80, hyp_len=18092, ref_len=22551)   
     Epoch 1.0000 dev Loss: 2.222 (words=72283, words/sec=39.45, time=0-03:35:03)   
     Epoch 1.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 1.0190: train_loss/word=1.996 (words=71154, words/sec=234.74, time=0-03:40:06)   
   Epoch 1.0380: train_loss/word=1.988 (words=139981, words/sec=332.46, time=0-03:43:33)   
   Epoch 1.0569: train_loss/word=1.984 (words=209102, words/sec=332.65, time=0-03:47:01)   
   Epoch 1.0759: train_loss/word=1.985 (words=280150, words/sec=332.93, time=0-03:50:34)   
   Epoch 1.0949: train_loss/word=1.984 (words=349486, words/sec=332.80, time=0-03:54:03)   
   Epoch 1.1139: train_loss/word=1.975 (words=420209, words/sec=333.61, time=0-03:57:35)   
   Epoch 1.1328: train_loss/word=1.972 (words=490419, words/sec=333.26, time=0-04:01:05)   
   Epoch 1.1518: train_loss/word=1.971 (words=561064, words/sec=333.30, time=0-04:04:37)   
   Epoch 1.1708: train_loss/word=1.967 (words=632092, words/sec=333.44, time=0-04:08:10)   
   Epoch 1.1898: train_loss/word=1.966 (words=701895, words/sec=332.91, time=0-04:11:40)   
   Epoch 1.2087: train_loss/word=1.966 (words=771770, words/sec=333.16, time=0-04:15:10)   
   Epoch 1.2277: train_loss/word=1.966 (words=838531, words/sec=332.24, time=0-04:18:31)   
   Epoch 1.2467: train_loss/word=1.964 (words=908449, words/sec=332.85, time=0-04:22:01)   
   Epoch 1.2657: train_loss/word=1.961 (words=976806, words/sec=333.16, time=0-04:25:26)   
   Epoch 1.2846: train_loss/word=1.960 (words=1047960, words/sec=333.01, time=0-04:29:00)   
   Epoch 1.3036: train_loss/word=1.959 (words=1115507, words/sec=332.42, time=0-04:32:23)   
   Epoch 1.3226: train_loss/word=1.959 (words=1184222, words/sec=332.66, time=0-04:35:49)   
   Epoch 1.3416: train_loss/word=1.958 (words=1254611, words/sec=333.06, time=0-04:39:21)   
   Epoch 1.3606: train_loss/word=1.957 (words=1323788, words/sec=332.65, time=0-04:42:49)   
   Epoch 1.3795: train_loss/word=1.954 (words=1394476, words/sec=333.13, time=0-04:46:21)   
   Epoch 1.3985: train_loss/word=1.953 (words=1463437, words/sec=332.85, time=0-04:49:48)   
   Epoch 1.4175: train_loss/word=1.951 (words=1532484, words/sec=332.92, time=0-04:53:15)   
   Epoch 1.4365: train_loss/word=1.948 (words=1604499, words/sec=333.35, time=0-04:56:51)   
   Epoch 1.4554: train_loss/word=1.947 (words=1676512, words/sec=333.68, time=0-05:00:27)   
   Epoch 1.4744: train_loss/word=1.947 (words=1745592, words/sec=333.29, time=0-05:03:55)   
   Epoch 1.4934: train_loss/word=1.945 (words=1815776, words/sec=333.32, time=0-05:07:25)   
   Epoch 1.5124: train_loss/word=1.943 (words=1885690, words/sec=332.99, time=0-05:10:55)   
   Epoch 1.5313: train_loss/word=1.941 (words=1953728, words/sec=332.98, time=0-05:14:19)   
   Epoch 1.5503: train_loss/word=1.940 (words=2027253, words/sec=333.68, time=0-05:18:00)   
   Epoch 1.5693: train_loss/word=1.939 (words=2095652, words/sec=332.84, time=0-05:21:25)   
   Epoch 1.5883: train_loss/word=1.937 (words=2167302, words/sec=333.84, time=0-05:25:00)   
   Epoch 1.6072: train_loss/word=1.936 (words=2239115, words/sec=333.67, time=0-05:28:35)   
   Epoch 1.6262: train_loss/word=1.935 (words=2309791, words/sec=332.99, time=0-05:32:07)   
   Epoch 1.6452: train_loss/word=1.934 (words=2378894, words/sec=332.60, time=0-05:35:35)   
   Epoch 1.6642: train_loss/word=1.932 (words=2448064, words/sec=332.82, time=0-05:39:03)   
   Epoch 1.6832: train_loss/word=1.931 (words=2518157, words/sec=333.10, time=0-05:42:33)   
   Epoch 1.7021: train_loss/word=1.930 (words=2588591, words/sec=333.35, time=0-05:46:05)   
   Epoch 1.7211: train_loss/word=1.929 (words=2659517, words/sec=333.41, time=0-05:49:37)   
   Epoch 1.7401: train_loss/word=1.928 (words=2729998, words/sec=333.06, time=0-05:53:09)   
   Epoch 1.7591: train_loss/word=1.925 (words=2797354, words/sec=332.26, time=0-05:56:32)   
   Epoch 1.7780: train_loss/word=1.924 (words=2869386, words/sec=333.61, time=0-06:00:08)   
   Epoch 1.7970: train_loss/word=1.922 (words=2938301, words/sec=332.78, time=0-06:03:35)   
   Epoch 1.8160: train_loss/word=1.919 (words=3008940, words/sec=333.24, time=0-06:07:07)   
   Epoch 1.8350: train_loss/word=1.917 (words=3078736, words/sec=333.54, time=0-06:10:36)   
   Epoch 1.8539: train_loss/word=1.916 (words=3150193, words/sec=333.21, time=0-06:14:10)   
   Epoch 1.8729: train_loss/word=1.915 (words=3216464, words/sec=332.42, time=0-06:17:30)   
   Epoch 1.8919: train_loss/word=1.914 (words=3288316, words/sec=333.06, time=0-06:21:05)   
   Epoch 1.9109: train_loss/word=1.912 (words=3358736, words/sec=332.84, time=0-06:24:37)   
   Epoch 1.9298: train_loss/word=1.911 (words=3426814, words/sec=332.06, time=0-06:28:02)   
   Epoch 1.9488: train_loss/word=1.909 (words=3494664, words/sec=332.35, time=0-06:31:26)   
   Epoch 1.9678: train_loss/word=1.907 (words=3563355, words/sec=332.80, time=0-06:34:53)   
   Epoch 1.9868: train_loss/word=1.905 (words=3633586, words/sec=333.26, time=0-06:38:23)   
   Epoch 2.0000: train_loss/word=1.904 (words=3683901, words/sec=334.00, time=0-06:40:54)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 2.0000 dev [auxiliary] BLEU4: 0.0282071221448, 0.330870/0.067419/0.016520/0.005748 (BP = 0.739378, ratio=0.77, hyp_len=17321, ref_len=22551)   
     Epoch 2.0000 dev Loss: 2.013 (words=72283, words/sec=42.72, time=0-07:09:06)   
     Epoch 2.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 2.0190: train_loss/word=1.707 (words=67923, words/sec=213.52, time=0-07:14:24)   
   Epoch 2.0380: train_loss/word=1.719 (words=139423, words/sec=333.74, time=0-07:17:58)   
   Epoch 2.0569: train_loss/word=1.718 (words=210329, words/sec=333.21, time=0-07:21:31)   
   Epoch 2.0759: train_loss/word=1.709 (words=279220, words/sec=332.14, time=0-07:24:58)   
   Epoch 2.0949: train_loss/word=1.710 (words=348799, words/sec=332.93, time=0-07:28:27)   
   Epoch 2.1139: train_loss/word=1.713 (words=417850, words/sec=332.52, time=0-07:31:55)   
   Epoch 2.1328: train_loss/word=1.714 (words=487182, words/sec=332.53, time=0-07:35:24)   
   Epoch 2.1518: train_loss/word=1.715 (words=558321, words/sec=333.05, time=0-07:38:57)   
   Epoch 2.1708: train_loss/word=1.714 (words=629584, words/sec=333.51, time=0-07:42:31)   
   Epoch 2.1898: train_loss/word=1.714 (words=700165, words/sec=333.11, time=0-07:46:03)   
   Epoch 2.2087: train_loss/word=1.715 (words=768280, words/sec=332.95, time=0-07:49:27)   
   Epoch 2.2277: train_loss/word=1.716 (words=837257, words/sec=332.74, time=0-07:52:55)   
   Epoch 2.2467: train_loss/word=1.716 (words=908170, words/sec=333.40, time=0-07:56:27)   
   Epoch 2.2657: train_loss/word=1.719 (words=978317, words/sec=333.19, time=0-07:59:58)   
   Epoch 2.2846: train_loss/word=1.721 (words=1049705, words/sec=333.07, time=0-08:03:32)   
   Epoch 2.3036: train_loss/word=1.724 (words=1120443, words/sec=333.33, time=0-08:07:04)   
   Epoch 2.3226: train_loss/word=1.725 (words=1191148, words/sec=333.35, time=0-08:10:36)   
   Epoch 2.3416: train_loss/word=1.725 (words=1258954, words/sec=332.70, time=0-08:14:00)   
   Epoch 2.3606: train_loss/word=1.725 (words=1328990, words/sec=332.82, time=0-08:17:31)   
   Epoch 2.3795: train_loss/word=1.723 (words=1398407, words/sec=332.58, time=0-08:20:59)   
   Epoch 2.3985: train_loss/word=1.723 (words=1468850, words/sec=333.29, time=0-08:24:31)   
   Epoch 2.4175: train_loss/word=1.723 (words=1538941, words/sec=332.93, time=0-08:28:01)   
   Epoch 2.4365: train_loss/word=1.723 (words=1607445, words/sec=332.37, time=0-08:31:27)   
   Epoch 2.4554: train_loss/word=1.723 (words=1677708, words/sec=332.94, time=0-08:34:58)   
   Epoch 2.4744: train_loss/word=1.724 (words=1746191, words/sec=332.97, time=0-08:38:24)   
   Epoch 2.4934: train_loss/word=1.726 (words=1820096, words/sec=333.29, time=0-08:42:06)   
   Epoch 2.5124: train_loss/word=1.727 (words=1890406, words/sec=333.26, time=0-08:45:37)   
   Epoch 2.5313: train_loss/word=1.726 (words=1961051, words/sec=333.19, time=0-08:49:09)   
   Epoch 2.5503: train_loss/word=1.727 (words=2031874, words/sec=333.29, time=0-08:52:41)   
   Epoch 2.5693: train_loss/word=1.727 (words=2100696, words/sec=332.91, time=0-08:56:08)   
   Epoch 2.5883: train_loss/word=1.727 (words=2168028, words/sec=332.67, time=0-08:59:31)   
   Epoch 2.6072: train_loss/word=1.727 (words=2238200, words/sec=333.11, time=0-09:03:01)   
   Epoch 2.6262: train_loss/word=1.726 (words=2309810, words/sec=333.41, time=0-09:06:36)   
   Epoch 2.6452: train_loss/word=1.725 (words=2381352, words/sec=333.17, time=0-09:10:11)   
   Epoch 2.6642: train_loss/word=1.724 (words=2451371, words/sec=332.92, time=0-09:13:41)   
   Epoch 2.6832: train_loss/word=1.723 (words=2520556, words/sec=332.35, time=0-09:17:09)   
   Epoch 2.7021: train_loss/word=1.722 (words=2591198, words/sec=332.86, time=0-09:20:41)   
   Epoch 2.7211: train_loss/word=1.722 (words=2661578, words/sec=332.89, time=0-09:24:13)   
   Epoch 2.7401: train_loss/word=1.722 (words=2731725, words/sec=333.69, time=0-09:27:43)   
   Epoch 2.7591: train_loss/word=1.722 (words=2803424, words/sec=333.34, time=0-09:31:18)   
   Epoch 2.7780: train_loss/word=1.721 (words=2873815, words/sec=332.37, time=0-09:34:50)   
   Epoch 2.7970: train_loss/word=1.721 (words=2942168, words/sec=332.10, time=0-09:38:16)   
   Epoch 2.8160: train_loss/word=1.720 (words=3011252, words/sec=332.22, time=0-09:41:44)   
   Epoch 2.8350: train_loss/word=1.720 (words=3080310, words/sec=332.19, time=0-09:45:12)   
   Epoch 2.8539: train_loss/word=1.721 (words=3149443, words/sec=332.59, time=0-09:48:39)   
   Epoch 2.8729: train_loss/word=1.721 (words=3217756, words/sec=332.31, time=0-09:52:05)   
   Epoch 2.8919: train_loss/word=1.721 (words=3287036, words/sec=332.83, time=0-09:55:33)   
   Epoch 2.9109: train_loss/word=1.721 (words=3356674, words/sec=332.43, time=0-09:59:03)   
   Epoch 2.9298: train_loss/word=1.721 (words=3424847, words/sec=332.69, time=0-10:02:28)   
   Epoch 2.9488: train_loss/word=1.721 (words=3495036, words/sec=333.25, time=0-10:05:58)   
   Epoch 2.9678: train_loss/word=1.721 (words=3565065, words/sec=333.51, time=0-10:09:28)   
   Epoch 2.9868: train_loss/word=1.720 (words=3635707, words/sec=333.60, time=0-10:13:00)   
   Epoch 3.0000: train_loss/word=1.720 (words=3683901, words/sec=332.88, time=0-10:15:25)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 3.0000 dev [auxiliary] BLEU4: 0.0437809534151, 0.375343/0.095440/0.030056/0.012067 (BP = 0.729226, ratio=0.76, hyp_len=17139, ref_len=22551)   
     Epoch 3.0000 dev Loss: 1.954 (words=72283, words/sec=43.24, time=0-10:43:16)   
     Epoch 3.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 3.0190: train_loss/word=1.590 (words=68173, words/sec=224.01, time=0-10:48:21)   
   Epoch 3.0380: train_loss/word=1.615 (words=139493, words/sec=333.64, time=0-10:51:55)   
   Epoch 3.0569: train_loss/word=1.603 (words=209747, words/sec=333.26, time=0-10:55:25)   
   Epoch 3.0759: train_loss/word=1.602 (words=279970, words/sec=333.52, time=0-10:58:56)   
   Epoch 3.0949: train_loss/word=1.601 (words=349850, words/sec=333.31, time=0-11:02:26)   
   Epoch 3.1139: train_loss/word=1.598 (words=420105, words/sec=333.62, time=0-11:05:56)   
   Epoch 3.1328: train_loss/word=1.599 (words=487557, words/sec=332.93, time=0-11:09:19)   
   Epoch 3.1518: train_loss/word=1.603 (words=556545, words/sec=332.98, time=0-11:12:46)   
   Epoch 3.1708: train_loss/word=1.603 (words=626433, words/sec=333.09, time=0-11:16:16)   
   Epoch 3.1898: train_loss/word=1.605 (words=695419, words/sec=332.52, time=0-11:19:43)   
   Epoch 3.2087: train_loss/word=1.604 (words=765342, words/sec=330.65, time=0-11:23:15)   
   Epoch 3.2277: train_loss/word=1.605 (words=837040, words/sec=331.69, time=0-11:26:51)   
   Epoch 3.2467: train_loss/word=1.598 (words=906370, words/sec=331.20, time=0-11:30:20)   
   Epoch 3.2657: train_loss/word=1.594 (words=976992, words/sec=331.33, time=0-11:33:53)   
   Epoch 3.2846: train_loss/word=1.591 (words=1045050, words/sec=330.99, time=0-11:37:19)   
   Epoch 3.3036: train_loss/word=1.590 (words=1112768, words/sec=330.71, time=0-11:40:44)   
   Epoch 3.3226: train_loss/word=1.591 (words=1183271, words/sec=330.87, time=0-11:44:17)   
   Epoch 3.3416: train_loss/word=1.592 (words=1254090, words/sec=331.37, time=0-11:47:51)   
   Epoch 3.3606: train_loss/word=1.591 (words=1323588, words/sec=329.93, time=0-11:51:21)   
   Epoch 3.3795: train_loss/word=1.592 (words=1394540, words/sec=330.01, time=0-11:54:56)   
   Epoch 3.3985: train_loss/word=1.592 (words=1464974, words/sec=329.94, time=0-11:58:30)   
   Epoch 3.4175: train_loss/word=1.593 (words=1534157, words/sec=329.72, time=0-12:01:59)   
   Epoch 3.4365: train_loss/word=1.593 (words=1605485, words/sec=329.99, time=0-12:05:36)   
   Epoch 3.4554: train_loss/word=1.593 (words=1673667, words/sec=329.00, time=0-12:09:03)   
   Epoch 3.4744: train_loss/word=1.593 (words=1743688, words/sec=329.85, time=0-12:12:35)   
   Epoch 3.4934: train_loss/word=1.594 (words=1813167, words/sec=329.89, time=0-12:16:06)   
   Epoch 3.5124: train_loss/word=1.595 (words=1884055, words/sec=330.59, time=0-12:19:40)   
   Epoch 3.5313: train_loss/word=1.595 (words=1954050, words/sec=330.83, time=0-12:23:12)   
   Epoch 3.5503: train_loss/word=1.596 (words=2023260, words/sec=331.01, time=0-12:26:41)   
   Epoch 3.5693: train_loss/word=1.596 (words=2093401, words/sec=331.16, time=0-12:30:13)   
   Epoch 3.5883: train_loss/word=1.596 (words=2165562, words/sec=331.95, time=0-12:33:50)   
   Epoch 3.6072: train_loss/word=1.595 (words=2235154, words/sec=330.75, time=0-12:37:20)   
   Epoch 3.6262: train_loss/word=1.596 (words=2304122, words/sec=330.86, time=0-12:40:49)   
   Epoch 3.6452: train_loss/word=1.597 (words=2373925, words/sec=330.28, time=0-12:44:20)   
   Epoch 3.6642: train_loss/word=1.598 (words=2447156, words/sec=330.03, time=0-12:48:02)   
   Epoch 3.6832: train_loss/word=1.598 (words=2514339, words/sec=329.61, time=0-12:51:26)   
   Epoch 3.7021: train_loss/word=1.598 (words=2583568, words/sec=329.93, time=0-12:54:56)   
   Epoch 3.7211: train_loss/word=1.597 (words=2653920, words/sec=329.94, time=0-12:58:29)   
   Epoch 3.7401: train_loss/word=1.595 (words=2722118, words/sec=329.58, time=0-13:01:56)   
   Epoch 3.7591: train_loss/word=1.593 (words=2790215, words/sec=329.52, time=0-13:05:23)   
   Epoch 3.7780: train_loss/word=1.592 (words=2861868, words/sec=330.06, time=0-13:09:00)   
   Epoch 3.7970: train_loss/word=1.591 (words=2934833, words/sec=330.59, time=0-13:12:40)   
   Epoch 3.8160: train_loss/word=1.589 (words=3005486, words/sec=331.00, time=0-13:16:14)   
   Epoch 3.8350: train_loss/word=1.589 (words=3073853, words/sec=330.69, time=0-13:19:41)   
   Epoch 3.8539: train_loss/word=1.588 (words=3145700, words/sec=331.47, time=0-13:23:17)   
   Epoch 3.8729: train_loss/word=1.587 (words=3215969, words/sec=331.71, time=0-13:26:49)   
   Epoch 3.8919: train_loss/word=1.586 (words=3285422, words/sec=331.44, time=0-13:30:19)   
   Epoch 3.9109: train_loss/word=1.586 (words=3354635, words/sec=331.13, time=0-13:33:48)   
   Epoch 3.9298: train_loss/word=1.586 (words=3425183, words/sec=331.08, time=0-13:37:21)   
   Epoch 3.9488: train_loss/word=1.586 (words=3496124, words/sec=331.59, time=0-13:40:55)   
   Epoch 3.9678: train_loss/word=1.586 (words=3567769, words/sec=330.31, time=0-13:44:32)   
   Epoch 3.9868: train_loss/word=1.586 (words=3636606, words/sec=329.44, time=0-13:48:01)   
   Epoch 4.0000: train_loss/word=1.586 (words=3683901, words/sec=328.90, time=0-13:50:24)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 4.0000 dev [auxiliary] BLEU4: 0.0534702538783, 0.359996/0.099720/0.035605/0.015044 (BP = 0.807465, ratio=0.82, hyp_len=18578, ref_len=22551)   
     Epoch 4.0000 dev Loss: 1.844 (words=72283, words/sec=31.34, time=0-14:28:51)   
     Epoch 4.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 4.0190: train_loss/word=1.435 (words=69309, words/sec=204.12, time=0-14:34:30)   
   Epoch 4.0380: train_loss/word=1.446 (words=139204, words/sec=331.31, time=0-14:38:01)   
   Epoch 4.0569: train_loss/word=1.458 (words=209444, words/sec=330.65, time=0-14:41:33)   
   Epoch 4.0759: train_loss/word=1.457 (words=277804, words/sec=330.19, time=0-14:45:00)   
   Epoch 4.0949: train_loss/word=1.458 (words=347401, words/sec=330.69, time=0-14:48:31)   
   Epoch 4.1139: train_loss/word=1.462 (words=415357, words/sec=329.82, time=0-14:51:57)   
   Epoch 4.1328: train_loss/word=1.463 (words=485019, words/sec=329.34, time=0-14:55:29)   
   Epoch 4.1518: train_loss/word=1.466 (words=553439, words/sec=329.40, time=0-14:58:56)   
   Epoch 4.1708: train_loss/word=1.468 (words=622302, words/sec=329.46, time=0-15:02:25)   
   Epoch 4.1898: train_loss/word=1.472 (words=692785, words/sec=329.71, time=0-15:05:59)   
   Epoch 4.2087: train_loss/word=1.473 (words=762706, words/sec=329.60, time=0-15:09:31)   
   Epoch 4.2277: train_loss/word=1.475 (words=833508, words/sec=329.79, time=0-15:13:06)   
   Epoch 4.2467: train_loss/word=1.476 (words=903825, words/sec=329.51, time=0-15:16:39)   
   Epoch 4.2657: train_loss/word=1.477 (words=974471, words/sec=329.83, time=0-15:20:13)   
   Epoch 4.2846: train_loss/word=1.478 (words=1044444, words/sec=329.97, time=0-15:23:45)   
   Epoch 4.3036: train_loss/word=1.480 (words=1115689, words/sec=330.63, time=0-15:27:21)   
   Epoch 4.3226: train_loss/word=1.482 (words=1185825, words/sec=330.67, time=0-15:30:53)   
   Epoch 4.3416: train_loss/word=1.483 (words=1254713, words/sec=330.23, time=0-15:34:22)   
   Epoch 4.3606: train_loss/word=1.482 (words=1323823, words/sec=330.18, time=0-15:37:51)   
   Epoch 4.3795: train_loss/word=1.479 (words=1391988, words/sec=330.75, time=0-15:41:17)   
   Epoch 4.3985: train_loss/word=1.477 (words=1463058, words/sec=331.30, time=0-15:44:52)   
   Epoch 4.4175: train_loss/word=1.475 (words=1532002, words/sec=330.82, time=0-15:48:20)   
   Epoch 4.4365: train_loss/word=1.473 (words=1603393, words/sec=330.58, time=0-15:51:56)   
   Epoch 4.4554: train_loss/word=1.471 (words=1672386, words/sec=329.81, time=0-15:55:25)   
   Epoch 4.4744: train_loss/word=1.470 (words=1742121, words/sec=329.32, time=0-15:58:57)   
   Epoch 4.4934: train_loss/word=1.469 (words=1811523, words/sec=329.51, time=0-16:02:28)   
   Epoch 4.5124: train_loss/word=1.468 (words=1881100, words/sec=329.25, time=0-16:05:59)   
   Epoch 4.5313: train_loss/word=1.468 (words=1952446, words/sec=329.37, time=0-16:09:35)   
   Epoch 4.5503: train_loss/word=1.469 (words=2023130, words/sec=329.54, time=0-16:13:10)   
   Epoch 4.5693: train_loss/word=1.469 (words=2092808, words/sec=329.40, time=0-16:16:41)   
   Epoch 4.5883: train_loss/word=1.469 (words=2163658, words/sec=329.57, time=0-16:20:16)   
   Epoch 4.6072: train_loss/word=1.470 (words=2232079, words/sec=329.14, time=0-16:23:44)   
   Epoch 4.6262: train_loss/word=1.470 (words=2303150, words/sec=330.85, time=0-16:27:19)   
   Epoch 4.6452: train_loss/word=1.470 (words=2374140, words/sec=330.99, time=0-16:30:54)   
   Epoch 4.6642: train_loss/word=1.471 (words=2446770, words/sec=330.85, time=0-16:34:33)   
   Epoch 4.6832: train_loss/word=1.471 (words=2513972, words/sec=329.87, time=0-16:37:57)   
   Epoch 4.7021: train_loss/word=1.472 (words=2584434, words/sec=330.56, time=0-16:41:30)   
   Epoch 4.7211: train_loss/word=1.473 (words=2653773, words/sec=330.75, time=0-16:45:00)   
   Epoch 4.7401: train_loss/word=1.473 (words=2722356, words/sec=330.46, time=0-16:48:27)   
   Epoch 4.7591: train_loss/word=1.473 (words=2793717, words/sec=330.42, time=0-16:52:03)   
   Epoch 4.7780: train_loss/word=1.474 (words=2864039, words/sec=329.53, time=0-16:55:37)   
   Epoch 4.7970: train_loss/word=1.474 (words=2934964, words/sec=329.32, time=0-16:59:12)   
   Epoch 4.8160: train_loss/word=1.475 (words=3005235, words/sec=329.02, time=0-17:02:46)   
   Epoch 4.8350: train_loss/word=1.475 (words=3076509, words/sec=329.40, time=0-17:06:22)   
   Epoch 4.8539: train_loss/word=1.476 (words=3147831, words/sec=329.30, time=0-17:09:59)   
   Epoch 4.8729: train_loss/word=1.477 (words=3218562, words/sec=329.47, time=0-17:13:33)   
   Epoch 4.8919: train_loss/word=1.478 (words=3285628, words/sec=328.42, time=0-17:16:57)   
   Epoch 4.9109: train_loss/word=1.478 (words=3354141, words/sec=329.74, time=0-17:20:25)   
   Epoch 4.9298: train_loss/word=1.479 (words=3421145, words/sec=330.21, time=0-17:23:48)   
   Epoch 4.9488: train_loss/word=1.480 (words=3491309, words/sec=331.36, time=0-17:27:20)   
   Epoch 4.9678: train_loss/word=1.480 (words=3564588, words/sec=331.89, time=0-17:31:01)   
   Epoch 4.9868: train_loss/word=1.481 (words=3636074, words/sec=330.14, time=0-17:34:37)   
   Epoch 5.0000: train_loss/word=1.481 (words=3683901, words/sec=329.40, time=0-17:37:02)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 5.0000 dev [auxiliary] BLEU4: 0.0560091522159, 0.366278/0.102257/0.036789/0.015909 (BP = 0.818548, ratio=0.83, hyp_len=18789, ref_len=22551)   
     Epoch 5.0000 dev Loss: 1.829 (words=72283, words/sec=30.27, time=0-18:16:50)   
     Epoch 5.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 5.0190: train_loss/word=1.369 (words=71024, words/sec=201.23, time=0-18:22:43)   
   Epoch 5.0380: train_loss/word=1.380 (words=143658, words/sec=331.61, time=0-18:26:22)   
   Epoch 5.0569: train_loss/word=1.388 (words=213373, words/sec=329.56, time=0-18:29:54)   
   Epoch 5.0759: train_loss/word=1.394 (words=283541, words/sec=329.83, time=0-18:33:26)   
   Epoch 5.0949: train_loss/word=1.396 (words=352832, words/sec=329.81, time=0-18:36:56)   
   Epoch 5.1139: train_loss/word=1.398 (words=423857, words/sec=329.70, time=0-18:40:32)   
   Epoch 5.1328: train_loss/word=1.398 (words=495596, words/sec=329.53, time=0-18:44:09)   
   Epoch 5.1518: train_loss/word=1.398 (words=564604, words/sec=329.41, time=0-18:47:39)   
   Epoch 5.1708: train_loss/word=1.396 (words=633292, words/sec=328.85, time=0-18:51:08)   
   Epoch 5.1898: train_loss/word=1.394 (words=701097, words/sec=329.29, time=0-18:54:34)   
   Epoch 5.2087: train_loss/word=1.395 (words=773268, words/sec=330.86, time=0-18:58:12)   
   Epoch 5.2277: train_loss/word=1.397 (words=845057, words/sec=331.39, time=0-19:01:49)   
   Epoch 5.2467: train_loss/word=1.399 (words=915064, words/sec=330.94, time=0-19:05:20)   
   Epoch 5.2657: train_loss/word=1.400 (words=987216, words/sec=331.70, time=0-19:08:58)   
   Epoch 5.2846: train_loss/word=1.400 (words=1058898, words/sec=331.39, time=0-19:12:34)   
   Epoch 5.3036: train_loss/word=1.401 (words=1126055, words/sec=330.66, time=0-19:15:57)   
   Epoch 5.3226: train_loss/word=1.403 (words=1194343, words/sec=330.73, time=0-19:19:23)   
   Epoch 5.3416: train_loss/word=1.404 (words=1266436, words/sec=331.53, time=0-19:23:01)   
   Epoch 5.3606: train_loss/word=1.404 (words=1337030, words/sec=331.04, time=0-19:26:34)   
   Epoch 5.3795: train_loss/word=1.407 (words=1407049, words/sec=329.94, time=0-19:30:06)   
   Epoch 5.3985: train_loss/word=1.407 (words=1479617, words/sec=329.18, time=0-19:33:47)   
   Epoch 5.4175: train_loss/word=1.408 (words=1548684, words/sec=328.66, time=0-19:37:17)   
   Epoch 5.4365: train_loss/word=1.409 (words=1619072, words/sec=328.91, time=0-19:40:51)   
   Epoch 5.4554: train_loss/word=1.410 (words=1687160, words/sec=328.96, time=0-19:44:18)   
   Epoch 5.4744: train_loss/word=1.411 (words=1756444, words/sec=329.37, time=0-19:47:48)   
   Epoch 5.4934: train_loss/word=1.413 (words=1826650, words/sec=329.26, time=0-19:51:22)   
   Epoch 5.5124: train_loss/word=1.415 (words=1893147, words/sec=328.48, time=0-19:54:44)   
   Epoch 5.5313: train_loss/word=1.416 (words=1962730, words/sec=330.06, time=0-19:58:15)   
   Epoch 5.5503: train_loss/word=1.417 (words=2031637, words/sec=331.03, time=0-20:01:43)   
   Epoch 5.5693: train_loss/word=1.418 (words=2101203, words/sec=331.31, time=0-20:05:13)   
   Epoch 5.5883: train_loss/word=1.418 (words=2170845, words/sec=330.72, time=0-20:08:44)   
   Epoch 5.6072: train_loss/word=1.419 (words=2242010, words/sec=330.56, time=0-20:12:19)   
   Epoch 5.6262: train_loss/word=1.419 (words=2311007, words/sec=330.65, time=0-20:15:47)   
   Epoch 5.6452: train_loss/word=1.419 (words=2379863, words/sec=331.21, time=0-20:19:15)   
   Epoch 5.6642: train_loss/word=1.419 (words=2450547, words/sec=331.19, time=0-20:22:49)   
   Epoch 5.6832: train_loss/word=1.418 (words=2519748, words/sec=330.25, time=0-20:26:18)   
   Epoch 5.7021: train_loss/word=1.415 (words=2589740, words/sec=329.41, time=0-20:29:51)   
   Epoch 5.7211: train_loss/word=1.413 (words=2659262, words/sec=329.63, time=0-20:33:22)   
   Epoch 5.7401: train_loss/word=1.411 (words=2728008, words/sec=328.83, time=0-20:36:51)   
   Epoch 5.7591: train_loss/word=1.410 (words=2797525, words/sec=328.78, time=0-20:40:22)   
   Epoch 5.7780: train_loss/word=1.409 (words=2866804, words/sec=328.77, time=0-20:43:53)   
   Epoch 5.7970: train_loss/word=1.407 (words=2935569, words/sec=329.37, time=0-20:47:22)   
   Epoch 5.8160: train_loss/word=1.406 (words=3003717, words/sec=329.10, time=0-20:50:49)   
   Epoch 5.8350: train_loss/word=1.405 (words=3075224, words/sec=330.14, time=0-20:54:25)   
   Epoch 5.8539: train_loss/word=1.403 (words=3145034, words/sec=329.73, time=0-20:57:57)   
   Epoch 5.8729: train_loss/word=1.403 (words=3214281, words/sec=329.46, time=0-21:01:27)   
   Epoch 5.8919: train_loss/word=1.402 (words=3284022, words/sec=329.68, time=0-21:04:59)   
   Epoch 5.9109: train_loss/word=1.401 (words=3355394, words/sec=330.58, time=0-21:08:35)   
   Epoch 5.9298: train_loss/word=1.401 (words=3425285, words/sec=330.05, time=0-21:12:06)   
   Epoch 5.9488: train_loss/word=1.400 (words=3496081, words/sec=330.84, time=0-21:15:40)   
   Epoch 5.9678: train_loss/word=1.400 (words=3564025, words/sec=330.34, time=0-21:19:06)   
   Epoch 5.9868: train_loss/word=1.399 (words=3634082, words/sec=331.11, time=0-21:22:38)   
   Epoch 6.0000: train_loss/word=1.399 (words=3683901, words/sec=331.10, time=0-21:25:08)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 6.0000 dev [auxiliary] BLEU4: 0.0594624666895, 0.368189/0.106229/0.039026/0.016922 (BP = 0.834083, ratio=0.85, hyp_len=19088, ref_len=22551)   
     Epoch 6.0000 dev Loss: 1.704 (words=72283, words/sec=28.80, time=0-22:06:58)   
     Epoch 6.0000: best dev score, writing model to examples/output/kftt-tree-wordlstm-2.mod   
   Epoch 6.0190: train_loss/word=1.244 (words=70498, words/sec=205.04, time=0-22:12:42)   
   Epoch 6.0380: train_loss/word=1.252 (words=138753, words/sec=331.65, time=0-22:16:08)   
   Epoch 6.0569: train_loss/word=1.251 (words=208208, words/sec=331.47, time=0-22:19:37)   
   Epoch 6.0759: train_loss/word=1.258 (words=279902, words/sec=331.63, time=0-22:23:13)   
   Epoch 6.0949: train_loss/word=1.260 (words=349075, words/sec=331.69, time=0-22:26:42)   
   Epoch 6.1139: train_loss/word=1.261 (words=419452, words/sec=331.27, time=0-22:30:14)   
   Epoch 6.1328: train_loss/word=1.265 (words=490615, words/sec=331.96, time=0-22:33:49)   
   Epoch 6.1518: train_loss/word=1.267 (words=562111, words/sec=330.82, time=0-22:37:25)   
   Epoch 6.1708: train_loss/word=1.269 (words=631600, words/sec=330.39, time=0-22:40:55)   
   Epoch 6.1898: train_loss/word=1.271 (words=701486, words/sec=330.20, time=0-22:44:27)   
   Epoch 6.2087: train_loss/word=1.273 (words=769833, words/sec=330.01, time=0-22:47:54)   
   Epoch 6.2277: train_loss/word=1.278 (words=840916, words/sec=330.27, time=0-22:51:29)   
   Epoch 6.2467: train_loss/word=1.281 (words=910647, words/sec=330.48, time=0-22:55:00)   
   Epoch 6.2657: train_loss/word=1.283 (words=980870, words/sec=330.29, time=0-22:58:33)   
   Epoch 6.2846: train_loss/word=1.285 (words=1052492, words/sec=330.48, time=0-23:02:09)   
   Epoch 6.3036: train_loss/word=1.287 (words=1120683, words/sec=329.85, time=0-23:05:36)   
   Epoch 6.3226: train_loss/word=1.290 (words=1192009, words/sec=330.44, time=0-23:09:12)   
   Epoch 6.3416: train_loss/word=1.290 (words=1264012, words/sec=330.92, time=0-23:12:50)   
   Epoch 6.3606: train_loss/word=1.292 (words=1334507, words/sec=330.62, time=0-23:16:23)   
   Epoch 6.3795: train_loss/word=1.293 (words=1405209, words/sec=333.16, time=0-23:19:55)   
   Epoch 6.3985: train_loss/word=1.295 (words=1476418, words/sec=333.68, time=0-23:23:28)   
   Epoch 6.4175: train_loss/word=1.296 (words=1546540, words/sec=333.20, time=0-23:26:59)   
   Epoch 6.4365: train_loss/word=1.298 (words=1617028, words/sec=333.20, time=0-23:30:30)   
   Epoch 6.4554: train_loss/word=1.300 (words=1685688, words/sec=332.89, time=0-23:33:57)   
   Epoch 6.4744: train_loss/word=1.303 (words=1754420, words/sec=332.31, time=0-23:37:23)   
   Epoch 6.4934: train_loss/word=1.305 (words=1823125, words/sec=333.03, time=0-23:40:50)   
   Epoch 6.5124: train_loss/word=1.307 (words=1892006, words/sec=332.95, time=0-23:44:17)   
   Epoch 6.5313: train_loss/word=1.310 (words=1963459, words/sec=333.62, time=0-23:47:51)   
   Epoch 6.5503: train_loss/word=1.311 (words=2032186, words/sec=333.20, time=0-23:51:17)   
   Epoch 6.5693: train_loss/word=1.312 (words=2101680, words/sec=332.99, time=0-23:54:46)   
   Epoch 6.5883: train_loss/word=1.316 (words=2171758, words/sec=333.36, time=0-23:58:16)   
   Epoch 6.6072: train_loss/word=1.317 (words=2241653, words/sec=333.01, time=1-00:01:46)   
   Epoch 6.6262: train_loss/word=1.319 (words=2312049, words/sec=332.42, time=1-00:05:18)   
   Epoch 6.6452: train_loss/word=1.320 (words=2382139, words/sec=332.72, time=1-00:08:48)   
   Epoch 6.6642: train_loss/word=1.322 (words=2451633, words/sec=332.47, time=1-00:12:17)   
   Epoch 6.6832: train_loss/word=1.323 (words=2519724, words/sec=332.29, time=1-00:15:42)   
   Epoch 6.7021: train_loss/word=1.325 (words=2588380, words/sec=332.46, time=1-00:19:09)   
   Epoch 6.7211: train_loss/word=1.326 (words=2659669, words/sec=333.28, time=1-00:22:43)   
   Epoch 6.7401: train_loss/word=1.328 (words=2727191, words/sec=332.17, time=1-00:26:06)   
   Epoch 6.7591: train_loss/word=1.329 (words=2798601, words/sec=332.70, time=1-00:29:41)   
   Epoch 6.7780: train_loss/word=1.330 (words=2866362, words/sec=331.93, time=1-00:33:05)   
   Epoch 6.7970: train_loss/word=1.331 (words=2935581, words/sec=332.65, time=1-00:36:33)   
   Epoch 6.8160: train_loss/word=1.331 (words=3005562, words/sec=333.09, time=1-00:40:03)   
   Epoch 6.8350: train_loss/word=1.332 (words=3074889, words/sec=332.97, time=1-00:43:31)   
   Epoch 6.8539: train_loss/word=1.333 (words=3146465, words/sec=333.22, time=1-00:47:06)   
   Epoch 6.8729: train_loss/word=1.334 (words=3215734, words/sec=333.23, time=1-00:50:34)   
   Epoch 6.8919: train_loss/word=1.335 (words=3284580, words/sec=332.88, time=1-00:54:01)   
   Epoch 6.9109: train_loss/word=1.336 (words=3355107, words/sec=333.40, time=1-00:57:32)   
   Epoch 6.9298: train_loss/word=1.337 (words=3423786, words/sec=332.72, time=1-01:00:59)   
   Epoch 6.9488: train_loss/word=1.338 (words=3495547, words/sec=331.99, time=1-01:04:35)   
   Epoch 6.9678: train_loss/word=1.339 (words=3564997, words/sec=331.14, time=1-01:08:04)   
   Epoch 6.9868: train_loss/word=1.339 (words=3635435, words/sec=331.34, time=1-01:11:37)   
   Epoch 7.0000: train_loss/word=1.340 (words=3683901, words/sec=331.55, time=1-01:14:03)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 7.0000 dev [auxiliary] BLEU4: 0.061622269668, 0.367242/0.103746/0.037413/0.016039 (BP = 0.891164, ratio=0.90, hyp_len=20221, ref_len=22551)   
     Epoch 7.0000 dev Loss: 1.795 (words=72283, words/sec=32.63, time=1-01:50:58)   
     new learning rate: 9.99999974738e-05   
     restarting trainer and reverting learned weights to best checkpoint..   
   Epoch 7.0190: train_loss/word=1.349 (words=69459, words/sec=266.24, time=1-01:55:19)   
   Epoch 7.0380: train_loss/word=1.369 (words=141398, words/sec=332.43, time=1-01:58:55)   
   Epoch 7.0569: train_loss/word=1.353 (words=210313, words/sec=330.48, time=1-02:02:24)   
   Epoch 7.0759: train_loss/word=1.347 (words=277759, words/sec=330.76, time=1-02:05:48)   
   Epoch 7.0949: train_loss/word=1.342 (words=348446, words/sec=331.35, time=1-02:09:21)   
   Epoch 7.1139: train_loss/word=1.340 (words=417469, words/sec=331.26, time=1-02:12:50)   
   Epoch 7.1328: train_loss/word=1.336 (words=488005, words/sec=331.52, time=1-02:16:22)   
   Epoch 7.1518: train_loss/word=1.333 (words=558979, words/sec=331.68, time=1-02:19:56)   
   Epoch 7.1708: train_loss/word=1.334 (words=631134, words/sec=332.15, time=1-02:23:34)   
   Epoch 7.1898: train_loss/word=1.330 (words=700460, words/sec=331.00, time=1-02:27:03)   
   Epoch 7.2087: train_loss/word=1.328 (words=769935, words/sec=331.51, time=1-02:30:33)   
   Epoch 7.2277: train_loss/word=1.328 (words=840288, words/sec=331.68, time=1-02:34:05)   
   Epoch 7.2467: train_loss/word=1.327 (words=909941, words/sec=331.18, time=1-02:37:35)   
   Epoch 7.2657: train_loss/word=1.327 (words=979749, words/sec=330.65, time=1-02:41:06)   
   Epoch 7.2846: train_loss/word=1.325 (words=1049813, words/sec=330.70, time=1-02:44:38)   
   Epoch 7.3036: train_loss/word=1.327 (words=1120243, words/sec=330.77, time=1-02:48:11)   
   Epoch 7.3226: train_loss/word=1.327 (words=1191132, words/sec=330.87, time=1-02:51:45)   
   Epoch 7.3416: train_loss/word=1.326 (words=1260763, words/sec=330.50, time=1-02:55:16)   
   Epoch 7.3606: train_loss/word=1.325 (words=1329491, words/sec=329.50, time=1-02:58:44)   
   Epoch 7.3795: train_loss/word=1.325 (words=1400337, words/sec=329.97, time=1-03:02:19)   
   Epoch 7.3985: train_loss/word=1.325 (words=1470241, words/sec=329.58, time=1-03:05:51)   
   Epoch 7.4175: train_loss/word=1.325 (words=1540954, words/sec=329.30, time=1-03:09:26)   
   Epoch 7.4365: train_loss/word=1.325 (words=1611014, words/sec=330.27, time=1-03:12:58)   
   Epoch 7.4554: train_loss/word=1.325 (words=1681131, words/sec=330.18, time=1-03:16:30)   
   Epoch 7.4744: train_loss/word=1.325 (words=1751391, words/sec=330.16, time=1-03:20:03)   
   Epoch 7.4934: train_loss/word=1.325 (words=1820835, words/sec=330.19, time=1-03:23:34)   
   Epoch 7.5124: train_loss/word=1.325 (words=1891579, words/sec=330.82, time=1-03:27:07)   
   Epoch 7.5313: train_loss/word=1.325 (words=1961398, words/sec=331.55, time=1-03:30:38)   
   Epoch 7.5503: train_loss/word=1.325 (words=2030187, words/sec=331.13, time=1-03:34:06)   
   Epoch 7.5693: train_loss/word=1.326 (words=2101971, words/sec=331.99, time=1-03:37:42)   
   Epoch 7.5883: train_loss/word=1.327 (words=2172754, words/sec=331.99, time=1-03:41:15)   
   Epoch 7.6072: train_loss/word=1.326 (words=2241784, words/sec=330.99, time=1-03:44:44)   
   Epoch 7.6262: train_loss/word=1.326 (words=2311455, words/sec=331.40, time=1-03:48:14)   
   Epoch 7.6452: train_loss/word=1.327 (words=2383083, words/sec=332.32, time=1-03:51:50)   
   Epoch 7.6642: train_loss/word=1.326 (words=2451776, words/sec=331.19, time=1-03:55:17)   
   Epoch 7.6832: train_loss/word=1.327 (words=2521730, words/sec=331.53, time=1-03:58:48)   
   Epoch 7.7021: train_loss/word=1.327 (words=2590329, words/sec=331.39, time=1-04:02:15)   
   Epoch 7.7211: train_loss/word=1.327 (words=2659828, words/sec=331.66, time=1-04:05:45)   
   Epoch 7.7401: train_loss/word=1.327 (words=2726933, words/sec=330.79, time=1-04:09:07)   
   Epoch 7.7591: train_loss/word=1.327 (words=2794742, words/sec=330.81, time=1-04:12:32)   
   Epoch 7.7780: train_loss/word=1.328 (words=2865660, words/sec=331.95, time=1-04:16:06)   
   Epoch 7.7970: train_loss/word=1.329 (words=2937590, words/sec=331.32, time=1-04:19:43)   
   Epoch 7.8160: train_loss/word=1.329 (words=3005778, words/sec=331.19, time=1-04:23:09)   
   Epoch 7.8350: train_loss/word=1.329 (words=3075894, words/sec=331.74, time=1-04:26:40)   
   Epoch 7.8539: train_loss/word=1.329 (words=3145262, words/sec=330.76, time=1-04:30:10)   
   Epoch 7.8729: train_loss/word=1.329 (words=3214260, words/sec=331.26, time=1-04:33:38)   
   Epoch 7.8919: train_loss/word=1.330 (words=3283677, words/sec=331.48, time=1-04:37:08)   
   Epoch 7.9109: train_loss/word=1.331 (words=3355369, words/sec=331.54, time=1-04:40:44)   
   Epoch 7.9298: train_loss/word=1.331 (words=3425659, words/sec=331.76, time=1-04:44:16)   
   Epoch 7.9488: train_loss/word=1.332 (words=3497086, words/sec=331.46, time=1-04:47:51)   
   Epoch 7.9678: train_loss/word=1.332 (words=3565637, words/sec=330.13, time=1-04:51:19)   
   Epoch 7.9868: train_loss/word=1.332 (words=3634588, words/sec=331.04, time=1-04:54:47)   
   Epoch 8.0000: train_loss/word=1.332 (words=3683901, words/sec=331.65, time=1-04:57:16)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 8.0000 dev [auxiliary] BLEU4: 0.0626603076563, 0.367794/0.104345/0.037755/0.017178 (BP = 0.887127, ratio=0.89, hyp_len=20139, ref_len=22551)   
     Epoch 8.0000 dev Loss: 1.759 (words=72283, words/sec=32.86, time=1-05:33:56)   
     new learning rate: 4.99999987369e-05   
     restarting trainer and reverting learned weights to best checkpoint..   
   Epoch 8.0190: train_loss/word=1.304 (words=69622, words/sec=282.62, time=1-05:38:02)   
   Epoch 8.0380: train_loss/word=1.284 (words=139014, words/sec=332.21, time=1-05:41:31)   
   Epoch 8.0569: train_loss/word=1.283 (words=206348, words/sec=331.43, time=1-05:44:54)   
   Epoch 8.0759: train_loss/word=1.285 (words=275052, words/sec=331.01, time=1-05:48:22)   
   Epoch 8.0949: train_loss/word=1.285 (words=345984, words/sec=331.68, time=1-05:51:56)   
   Epoch 8.1139: train_loss/word=1.285 (words=416254, words/sec=331.39, time=1-05:55:28)   
   Epoch 8.1328: train_loss/word=1.284 (words=483124, words/sec=330.19, time=1-05:58:50)   
   Epoch 8.1518: train_loss/word=1.281 (words=553250, words/sec=330.74, time=1-06:02:22)   
   Epoch 8.1708: train_loss/word=1.281 (words=624158, words/sec=331.16, time=1-06:05:56)   
   Epoch 8.1898: train_loss/word=1.283 (words=693868, words/sec=331.43, time=1-06:09:27)   
   Epoch 8.2087: train_loss/word=1.282 (words=764050, words/sec=331.42, time=1-06:12:59)   
   Epoch 8.2277: train_loss/word=1.282 (words=834637, words/sec=331.69, time=1-06:16:31)   
   Epoch 8.2467: train_loss/word=1.281 (words=905978, words/sec=331.76, time=1-06:20:06)   
   Epoch 8.2657: train_loss/word=1.280 (words=976951, words/sec=331.20, time=1-06:23:41)   
   Epoch 8.2846: train_loss/word=1.280 (words=1046195, words/sec=330.76, time=1-06:27:10)   
   Epoch 8.3036: train_loss/word=1.280 (words=1116816, words/sec=331.08, time=1-06:30:43)   
   Epoch 8.3226: train_loss/word=1.281 (words=1189923, words/sec=331.93, time=1-06:34:24)   
   Epoch 8.3416: train_loss/word=1.281 (words=1260580, words/sec=331.60, time=1-06:37:57)   
   Epoch 8.3606: train_loss/word=1.282 (words=1331313, words/sec=331.95, time=1-06:41:30)   
   Epoch 8.3795: train_loss/word=1.283 (words=1403723, words/sec=331.85, time=1-06:45:08)   
   Epoch 8.3985: train_loss/word=1.281 (words=1475149, words/sec=331.76, time=1-06:48:43)   
   Epoch 8.4175: train_loss/word=1.281 (words=1545242, words/sec=331.42, time=1-06:52:15)   
   Epoch 8.4365: train_loss/word=1.280 (words=1615491, words/sec=331.48, time=1-06:55:47)   
   Epoch 8.4554: train_loss/word=1.280 (words=1685656, words/sec=331.51, time=1-06:59:18)   
   Epoch 8.4744: train_loss/word=1.280 (words=1755337, words/sec=331.59, time=1-07:02:48)   
   Epoch 8.4934: train_loss/word=1.280 (words=1825963, words/sec=330.66, time=1-07:06:22)   
   Epoch 8.5124: train_loss/word=1.279 (words=1895637, words/sec=330.63, time=1-07:09:53)   
   Epoch 8.5313: train_loss/word=1.279 (words=1964790, words/sec=331.12, time=1-07:13:22)   
   Epoch 8.5503: train_loss/word=1.280 (words=2036038, words/sec=331.79, time=1-07:16:56)   
   Epoch 8.5693: train_loss/word=1.280 (words=2105402, words/sec=331.59, time=1-07:20:26)   
   Epoch 8.5883: train_loss/word=1.280 (words=2173193, words/sec=330.58, time=1-07:23:51)   
   Epoch 8.6072: train_loss/word=1.279 (words=2239292, words/sec=330.77, time=1-07:27:10)   
   Epoch 8.6262: train_loss/word=1.280 (words=2308088, words/sec=330.64, time=1-07:30:39)   
   Epoch 8.6452: train_loss/word=1.280 (words=2379862, words/sec=331.91, time=1-07:34:15)   
   Epoch 8.6642: train_loss/word=1.280 (words=2448820, words/sec=331.34, time=1-07:37:43)   
   Epoch 8.6832: train_loss/word=1.281 (words=2515967, words/sec=330.31, time=1-07:41:06)   
   Epoch 8.7021: train_loss/word=1.281 (words=2587328, words/sec=331.49, time=1-07:44:41)   
   Epoch 8.7211: train_loss/word=1.282 (words=2655281, words/sec=331.36, time=1-07:48:07)   
   Epoch 8.7401: train_loss/word=1.282 (words=2724601, words/sec=331.32, time=1-07:51:36)   
   Epoch 8.7591: train_loss/word=1.282 (words=2797474, words/sec=331.79, time=1-07:55:15)   
   Epoch 8.7780: train_loss/word=1.282 (words=2868959, words/sec=331.19, time=1-07:58:51)   
   Epoch 8.7970: train_loss/word=1.282 (words=2938635, words/sec=330.57, time=1-08:02:22)   
   Epoch 8.8160: train_loss/word=1.282 (words=3008998, words/sec=330.87, time=1-08:05:55)   
   Epoch 8.8350: train_loss/word=1.282 (words=3078539, words/sec=330.35, time=1-08:09:25)   
   Epoch 8.8539: train_loss/word=1.283 (words=3147307, words/sec=331.35, time=1-08:12:53)   
   Epoch 8.8729: train_loss/word=1.282 (words=3215376, words/sec=330.68, time=1-08:16:19)   
   Epoch 8.8919: train_loss/word=1.282 (words=3285269, words/sec=331.12, time=1-08:19:50)   
   Epoch 8.9109: train_loss/word=1.282 (words=3356454, words/sec=331.53, time=1-08:23:24)   
   Epoch 8.9298: train_loss/word=1.283 (words=3427343, words/sec=331.07, time=1-08:26:58)   
   Epoch 8.9488: train_loss/word=1.284 (words=3496002, words/sec=330.99, time=1-08:30:26)   
   Epoch 8.9678: train_loss/word=1.284 (words=3568286, words/sec=331.59, time=1-08:34:04)   
   Epoch 8.9868: train_loss/word=1.285 (words=3635969, words/sec=330.52, time=1-08:37:29)   
   Epoch 9.0000: train_loss/word=1.285 (words=3683901, words/sec=330.62, time=1-08:39:54)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 9.0000 dev [auxiliary] BLEU4: 0.0651745493867, 0.371757/0.107207/0.040132/0.018299 (BP = 0.886090, ratio=0.89, hyp_len=20118, ref_len=22551)   
     Epoch 9.0000 dev Loss: 1.735 (words=72283, words/sec=33.39, time=1-09:15:58)   
     new learning rate: 2.49999993684e-05   
     restarting trainer and reverting learned weights to best checkpoint..   
   Epoch 9.0190: train_loss/word=1.245 (words=69600, words/sec=282.24, time=1-09:20:05)   
   Epoch 9.0380: train_loss/word=1.245 (words=139789, words/sec=331.61, time=1-09:23:37)   
   Epoch 9.0569: train_loss/word=1.239 (words=207271, words/sec=330.70, time=1-09:27:01)   
   Epoch 9.0759: train_loss/word=1.239 (words=275736, words/sec=330.60, time=1-09:30:28)   
   Epoch 9.0949: train_loss/word=1.245 (words=347448, words/sec=331.47, time=1-09:34:04)   
   Epoch 9.1139: train_loss/word=1.250 (words=418517, words/sec=331.44, time=1-09:37:38)   
   Epoch 9.1328: train_loss/word=1.251 (words=490222, words/sec=331.60, time=1-09:41:15)   
   Epoch 9.1518: train_loss/word=1.252 (words=558819, words/sec=332.34, time=1-09:44:41)   
   Epoch 9.1708: train_loss/word=1.251 (words=627221, words/sec=332.35, time=1-09:48:07)   
   Epoch 9.1898: train_loss/word=1.251 (words=698414, words/sec=333.17, time=1-09:51:41)   
   Epoch 9.2087: train_loss/word=1.251 (words=768605, words/sec=333.35, time=1-09:55:11)   
   Epoch 9.2277: train_loss/word=1.252 (words=839500, words/sec=333.41, time=1-09:58:44)   
   Epoch 9.2467: train_loss/word=1.252 (words=908097, words/sec=333.38, time=1-10:02:10)   
   Epoch 9.2657: train_loss/word=1.252 (words=974817, words/sec=332.27, time=1-10:05:30)   
   Epoch 9.2846: train_loss/word=1.252 (words=1046059, words/sec=333.37, time=1-10:09:04)   
   Epoch 9.3036: train_loss/word=1.252 (words=1116768, words/sec=333.76, time=1-10:12:36)   
   Epoch 9.3226: train_loss/word=1.252 (words=1187653, words/sec=334.04, time=1-10:16:08)   
   Epoch 9.3416: train_loss/word=1.252 (words=1256566, words/sec=333.58, time=1-10:19:35)   
   Epoch 9.3606: train_loss/word=1.253 (words=1326967, words/sec=333.93, time=1-10:23:06)   
   Epoch 9.3795: train_loss/word=1.253 (words=1397736, words/sec=333.84, time=1-10:26:38)   
   Epoch 9.3985: train_loss/word=1.254 (words=1469230, words/sec=334.12, time=1-10:30:11)   
   Epoch 9.4175: train_loss/word=1.252 (words=1538624, words/sec=333.22, time=1-10:33:40)   
   Epoch 9.4365: train_loss/word=1.252 (words=1610115, words/sec=333.91, time=1-10:37:14)   
   Epoch 9.4554: train_loss/word=1.252 (words=1679573, words/sec=333.87, time=1-10:40:42)   
   Epoch 9.4744: train_loss/word=1.253 (words=1749181, words/sec=333.55, time=1-10:44:11)   
   Epoch 9.4934: train_loss/word=1.253 (words=1818650, words/sec=333.51, time=1-10:47:39)   
   Epoch 9.5124: train_loss/word=1.253 (words=1888987, words/sec=333.50, time=1-10:51:10)   
   Epoch 9.5313: train_loss/word=1.253 (words=1956648, words/sec=333.66, time=1-10:54:33)   
   Epoch 9.5503: train_loss/word=1.252 (words=2026007, words/sec=333.09, time=1-10:58:01)   
   Epoch 9.5693: train_loss/word=1.251 (words=2095861, words/sec=333.51, time=1-11:01:30)   
   Epoch 9.5883: train_loss/word=1.251 (words=2164513, words/sec=333.61, time=1-11:04:56)   
   Epoch 9.6072: train_loss/word=1.251 (words=2234582, words/sec=334.00, time=1-11:08:26)   
   Epoch 9.6262: train_loss/word=1.251 (words=2305961, words/sec=333.82, time=1-11:12:00)   
   Epoch 9.6452: train_loss/word=1.252 (words=2376315, words/sec=333.81, time=1-11:15:30)   
   Epoch 9.6642: train_loss/word=1.252 (words=2448120, words/sec=333.90, time=1-11:19:05)   
   Epoch 9.6832: train_loss/word=1.252 (words=2516589, words/sec=333.67, time=1-11:22:31)   
   Epoch 9.7021: train_loss/word=1.251 (words=2588023, words/sec=333.93, time=1-11:26:05)   
   Epoch 9.7211: train_loss/word=1.251 (words=2656165, words/sec=333.19, time=1-11:29:29)   
   Epoch 9.7401: train_loss/word=1.251 (words=2724885, words/sec=333.61, time=1-11:32:55)   
   Epoch 9.7591: train_loss/word=1.251 (words=2793646, words/sec=333.68, time=1-11:36:21)   
   Epoch 9.7780: train_loss/word=1.251 (words=2865538, words/sec=333.52, time=1-11:39:57)   
   Epoch 9.7970: train_loss/word=1.251 (words=2934503, words/sec=333.12, time=1-11:43:24)   
   Epoch 9.8160: train_loss/word=1.252 (words=3004805, words/sec=328.97, time=1-11:46:57)   
   Epoch 9.8350: train_loss/word=1.252 (words=3076579, words/sec=330.15, time=1-11:50:35)   
   Epoch 9.8539: train_loss/word=1.252 (words=3145157, words/sec=331.17, time=1-11:54:02)   
   Epoch 9.8729: train_loss/word=1.252 (words=3214344, words/sec=331.46, time=1-11:57:31)   
   Epoch 9.8919: train_loss/word=1.252 (words=3284784, words/sec=331.18, time=1-12:01:03)   
   Epoch 9.9109: train_loss/word=1.253 (words=3355939, words/sec=332.39, time=1-12:04:37)   
   Epoch 9.9298: train_loss/word=1.253 (words=3425515, words/sec=331.57, time=1-12:08:07)   
   Epoch 9.9488: train_loss/word=1.253 (words=3494960, words/sec=331.79, time=1-12:11:37)   
   Epoch 9.9678: train_loss/word=1.253 (words=3565262, words/sec=330.59, time=1-12:15:09)   
   Epoch 9.9868: train_loss/word=1.253 (words=3634309, words/sec=330.26, time=1-12:18:38)   
   Epoch 10.0000: train_loss/word=1.253 (words=3683901, words/sec=331.04, time=1-12:21:08)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 10.0000 dev [auxiliary] BLEU4: 0.0668203706379, 0.378109/0.112238/0.041831/0.018943 (BP = 0.877467, ratio=0.88, hyp_len=19944, ref_len=22551)   
     Epoch 10.0000 dev Loss: 1.710 (words=72283, words/sec=28.92, time=1-13:02:47)   
     Early stopping   
   reverting learned weights to best checkpoint..   
   > Evaluating test set   
     initialized PolynomialNormalization({'apply_during_search': True})     
     /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja     
     BLEU4: 0.0594624666895, 0.368189/0.106229/0.039026/0.016922 (BP = 0.834083, ratio=0.85, hyp_len=19088, ref_len=22551)     

Experiment                    | Final Scores                           
-----------------------------------------------------------------------
kftt-tree-wordlstm-2          | BLEU4: 0.0594624666895, 0.368189/0.106229/0.039026/0.016922 (BP = 0.834083, ratio=0.85, hyp_len=19088, ref_len=22551)
