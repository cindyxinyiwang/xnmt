Singularity: Invoking an interactive shell within container...

[0m[dynet] initializing CUDA
Request for 1 GPU ...
[dynet] Device Number: 0
[dynet]   Device name: GeForce GTX 1080 Ti
[dynet]   Memory Clock Rate (KHz): 5505000
[dynet]   Memory Bus Width (bits): 352
[dynet]   Peak Memory Bandwidth (GB/s): 484.44
[dynet]   Memory Free (GB): 11.5421/11.7151
[dynet]
[dynet] Device(s) selected: 0
[dynet] random seed: 2761618410
[dynet] using autobatching
[dynet] allocating memory: 10000MB
[dynet] memory allocation done.
=> Running kftt-tree-masktrain
   > Preprocessing   
   > Training   
   initialized BilingualTrainingCorpus({'train_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja', 'dev_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.en', 'dev_ref_file': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upper.en', 'dev_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja', 'train_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.en'})   
   initialized PlainTextReader({})   
   initialized TreeReader({})   
   initialized BilingualCorpusParser({'trg_reader': <xnmt.input.TreeReader object at 0x7f96bc6d60d0>, 'src_reader': <xnmt.input.PlainTextReader object at 0x7f96bc6d6090>})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   initialized SimpleWordEmbedder({'vocab_size': 48841, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>})   
   initialized LSTMSeqTransducer({'layers': 1, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>})   
   initialized StandardAttender({'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>})   
   initialized SimpleWordEmbedder({'vocab_size': 55991, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>})   
   initialized CopyBridge({'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>, 'dec_layers': 1})   
   initialized TreeDecoder({'layers': 1, 'bridge': <xnmt.decoder.CopyBridge object at 0x7f96a5b05b50>, 'vocab_size': 55991, 'mlp_hidden_dim': 512, 'word_lstm': True, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7f96bc6c4e90>})   
   initialized DefaultTranslator({'attender': <xnmt.attender.StandardAttender object at 0x7f96a5b051d0>, 'decoder': <xnmt.decoder.TreeDecoder object at 0x7f96a5b05890>, 'src_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7f96bb4e0310>, 'loop_trg': True, 'trg_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7f96a5b05b10>, 'encoder': <xnmt.lstm.LSTMSeqTransducer object at 0x7f96bb4e0350>})   
   initialized SrcBatcher({'batch_size': 4})   
   Epoch 0.0190: train_loss/word=2.610 (words=95156, words/sec=162.21, time=0-00:09:46)   
   Epoch 0.0380: train_loss/word=2.440 (words=185388, words/sec=150.78, time=0-00:19:45)   
   Epoch 0.0569: train_loss/word=2.295 (words=277952, words/sec=159.89, time=0-00:29:23)   
   Epoch 0.0759: train_loss/word=2.182 (words=370016, words/sec=160.30, time=0-00:38:58)   
   Epoch 0.0949: train_loss/word=2.095 (words=463952, words/sec=160.26, time=0-00:48:44)   
   Epoch 0.1139: train_loss/word=2.025 (words=552844, words/sec=161.47, time=0-00:57:54)   
   Epoch 0.1328: train_loss/word=1.971 (words=648080, words/sec=157.49, time=0-01:07:59)   
   Epoch 0.1518: train_loss/word=1.927 (words=739308, words/sec=160.81, time=0-01:17:27)   
   Epoch 0.1708: train_loss/word=1.888 (words=827232, words/sec=163.55, time=0-01:26:24)   
   Epoch 0.1898: train_loss/word=1.858 (words=915000, words/sec=158.15, time=0-01:35:39)   
   Epoch 0.2087: train_loss/word=1.831 (words=1005840, words/sec=156.00, time=0-01:45:21)   
   Epoch 0.2277: train_loss/word=1.806 (words=1095580, words/sec=162.47, time=0-01:54:34)   
   Epoch 0.2467: train_loss/word=1.784 (words=1190368, words/sec=157.28, time=0-02:04:36)   
   Epoch 0.2657: train_loss/word=1.765 (words=1280944, words/sec=156.81, time=0-02:14:14)   
   Epoch 0.2846: train_loss/word=1.747 (words=1371876, words/sec=156.47, time=0-02:23:55)   
   Epoch 0.3036: train_loss/word=1.730 (words=1467956, words/sec=154.98, time=0-02:34:15)   
   Epoch 0.3226: train_loss/word=1.717 (words=1558988, words/sec=155.46, time=0-02:44:01)   
   Epoch 0.3416: train_loss/word=1.702 (words=1648148, words/sec=155.84, time=0-02:53:33)   
   Epoch 0.3606: train_loss/word=1.689 (words=1737760, words/sec=157.53, time=0-03:03:02)   
   Epoch 0.3795: train_loss/word=1.677 (words=1830688, words/sec=156.25, time=0-03:12:56)   
   Epoch 0.3985: train_loss/word=1.664 (words=1922416, words/sec=151.83, time=0-03:23:01)   
   Epoch 0.4175: train_loss/word=1.651 (words=2018928, words/sec=145.49, time=0-03:34:04)   
   Epoch 0.4365: train_loss/word=1.642 (words=2109364, words/sec=150.56, time=0-03:44:05)   
   Epoch 0.4554: train_loss/word=1.632 (words=2197968, words/sec=155.86, time=0-03:53:33)   
   Epoch 0.4744: train_loss/word=1.623 (words=2292172, words/sec=156.40, time=0-04:03:35)   
   Epoch 0.4934: train_loss/word=1.615 (words=2380520, words/sec=155.27, time=0-04:13:04)   
   Epoch 0.5124: train_loss/word=1.607 (words=2472572, words/sec=153.33, time=0-04:23:05)   
   Epoch 0.5313: train_loss/word=1.598 (words=2564760, words/sec=156.83, time=0-04:32:52)   
   Epoch 0.5503: train_loss/word=1.589 (words=2653476, words/sec=159.83, time=0-04:42:08)   
   Epoch 0.5693: train_loss/word=1.581 (words=2743336, words/sec=152.01, time=0-04:51:59)   
   Epoch 0.5883: train_loss/word=1.574 (words=2835968, words/sec=158.08, time=0-05:01:45)   
   Epoch 0.6072: train_loss/word=1.567 (words=2929696, words/sec=154.37, time=0-05:11:52)   
   Epoch 0.6262: train_loss/word=1.560 (words=3020332, words/sec=156.19, time=0-05:21:32)   
   Epoch 0.6452: train_loss/word=1.554 (words=3108612, words/sec=154.06, time=0-05:31:05)   
   Epoch 0.6642: train_loss/word=1.548 (words=3194380, words/sec=158.09, time=0-05:40:08)   
   Epoch 0.6832: train_loss/word=1.541 (words=3286708, words/sec=160.45, time=0-05:49:43)   
   Epoch 0.7021: train_loss/word=1.535 (words=3377672, words/sec=158.36, time=0-05:59:18)   
   Epoch 0.7211: train_loss/word=1.528 (words=3473500, words/sec=161.19, time=0-06:09:12)   
   Epoch 0.7401: train_loss/word=1.522 (words=3570172, words/sec=158.82, time=0-06:19:21)   
   Epoch 0.7591: train_loss/word=1.517 (words=3655464, words/sec=156.06, time=0-06:28:27)   
   Epoch 0.7780: train_loss/word=1.512 (words=3753988, words/sec=154.31, time=0-06:39:06)   
   Epoch 0.7970: train_loss/word=1.507 (words=3845588, words/sec=157.69, time=0-06:48:47)   
   Epoch 0.8160: train_loss/word=1.502 (words=3939612, words/sec=152.29, time=0-06:59:04)   
   Epoch 0.8350: train_loss/word=1.497 (words=4032845, words/sec=149.73, time=0-07:09:27)   
   Epoch 0.8540: train_loss/word=1.492 (words=4118361, words/sec=148.84, time=0-07:19:01)   
   Epoch 0.8729: train_loss/word=1.488 (words=4207345, words/sec=152.38, time=0-07:28:45)   
   Epoch 0.8919: train_loss/word=1.482 (words=4303513, words/sec=150.10, time=0-07:39:26)   
   Epoch 0.9109: train_loss/word=1.478 (words=4394517, words/sec=148.04, time=0-07:49:41)   
   Epoch 0.9299: train_loss/word=1.472 (words=4491181, words/sec=151.16, time=0-08:00:20)   
   Epoch 0.9488: train_loss/word=1.468 (words=4580705, words/sec=146.84, time=0-08:10:30)   
   Epoch 0.9678: train_loss/word=1.464 (words=4670221, words/sec=143.27, time=0-08:20:55)   
   Epoch 0.9868: train_loss/word=1.459 (words=4764901, words/sec=145.43, time=0-08:31:46)   
   Epoch 1.0000: train_loss/word=1.456 (words=4825525, words/sec=148.43, time=0-08:38:34)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 1.0000 dev [auxiliary] Loss: 12.019   
     Epoch 1.0000 dev BLEU4: 0.026254100973, 0.340147/0.067991/0.018074/0.006620 (BP = 0.643716, ratio=0.69, hyp_len=15655, ref_len=22551) (words=90957, words/sec=40.80, time=0-09:15:44)   
     Epoch 1.0000: best dev score, writing model to examples/output/kftt-tree-masktrain.mod   
   Epoch 1.0190: train_loss/word=1.157 (words=94400, words/sec=128.19, time=0-09:28:00)   
   Epoch 1.0380: train_loss/word=1.137 (words=185540, words/sec=150.61, time=0-09:38:05)   
   Epoch 1.0569: train_loss/word=1.142 (words=280132, words/sec=147.67, time=0-09:48:46)   
   Epoch 1.0759: train_loss/word=1.130 (words=366328, words/sec=149.21, time=0-09:58:23)   
   Epoch 1.0949: train_loss/word=1.133 (words=453320, words/sec=147.51, time=0-10:08:13)   
   Epoch 1.1139: train_loss/word=1.128 (words=544612, words/sec=146.24, time=0-10:18:37)   
   Epoch 1.1328: train_loss/word=1.128 (words=639108, words/sec=142.13, time=0-10:29:42)   
   Epoch 1.1518: train_loss/word=1.122 (words=729784, words/sec=157.29, time=0-10:39:19)   
   Epoch 1.1708: train_loss/word=1.119 (words=820804, words/sec=158.13, time=0-10:48:54)   
   Epoch 1.1898: train_loss/word=1.119 (words=910524, words/sec=157.18, time=0-10:58:25)   
   Epoch 1.2087: train_loss/word=1.117 (words=1000400, words/sec=159.29, time=0-11:07:49)   
   Epoch 1.2277: train_loss/word=1.115 (words=1089649, words/sec=157.99, time=0-11:17:14)   
   Epoch 1.2467: train_loss/word=1.113 (words=1175465, words/sec=155.21, time=0-11:26:27)   
   Epoch 1.2657: train_loss/word=1.113 (words=1270757, words/sec=113.84, time=0-11:40:24)   
   Epoch 1.2847: train_loss/word=1.112 (words=1359381, words/sec=103.22, time=0-11:54:43)   
   Epoch 1.3036: train_loss/word=1.110 (words=1448969, words/sec=99.60, time=0-12:09:42)   
   Epoch 1.3226: train_loss/word=1.111 (words=1539145, words/sec=93.34, time=0-12:25:48)   
   Epoch 1.3416: train_loss/word=1.109 (words=1630113, words/sec=102.27, time=0-12:40:38)   
   Epoch 1.3606: train_loss/word=1.109 (words=1720781, words/sec=95.79, time=0-12:56:24)   
   Epoch 1.3795: train_loss/word=1.110 (words=1811805, words/sec=97.03, time=0-13:12:03)   
   Epoch 1.3985: train_loss/word=1.109 (words=1903493, words/sec=108.80, time=0-13:26:05)   
   Epoch 1.4175: train_loss/word=1.107 (words=1989749, words/sec=123.10, time=0-13:37:46)   
   Epoch 1.4365: train_loss/word=1.105 (words=2083513, words/sec=131.00, time=0-13:49:42)   
   Epoch 1.4555: train_loss/word=1.105 (words=2179809, words/sec=128.87, time=0-14:02:09)   
   Epoch 1.4744: train_loss/word=1.105 (words=2267461, words/sec=132.58, time=0-14:13:10)   
   Epoch 1.4934: train_loss/word=1.103 (words=2361585, words/sec=144.74, time=0-14:24:00)   
   Epoch 1.5124: train_loss/word=1.103 (words=2454037, words/sec=144.95, time=0-14:34:38)   
   Epoch 1.5314: train_loss/word=1.101 (words=2546609, words/sec=145.56, time=0-14:45:14)   
   Epoch 1.5503: train_loss/word=1.100 (words=2639269, words/sec=144.29, time=0-14:55:56)   
   Epoch 1.5693: train_loss/word=1.098 (words=2727109, words/sec=145.41, time=0-15:06:00)   
   Epoch 1.5883: train_loss/word=1.098 (words=2816237, words/sec=141.76, time=0-15:16:29)   
   Epoch 1.6073: train_loss/word=1.096 (words=2908089, words/sec=146.39, time=0-15:26:56)   
   Epoch 1.6262: train_loss/word=1.096 (words=3001825, words/sec=141.08, time=0-15:38:01)   
   Epoch 1.6452: train_loss/word=1.095 (words=3091869, words/sec=141.14, time=0-15:48:39)   
   Epoch 1.6642: train_loss/word=1.095 (words=3181265, words/sec=142.13, time=0-15:59:08)   
   Epoch 1.6832: train_loss/word=1.094 (words=3269889, words/sec=141.58, time=0-16:09:34)   
   Epoch 1.7021: train_loss/word=1.093 (words=3363113, words/sec=138.08, time=0-16:20:49)   
   Epoch 1.7211: train_loss/word=1.093 (words=3453589, words/sec=140.68, time=0-16:31:32)   
   Epoch 1.7401: train_loss/word=1.092 (words=3548389, words/sec=143.82, time=0-16:42:31)   
   Epoch 1.7591: train_loss/word=1.091 (words=3645317, words/sec=140.94, time=0-16:53:59)   
   Epoch 1.7781: train_loss/word=1.091 (words=3736153, words/sec=141.31, time=0-17:04:42)   
   Epoch 1.7970: train_loss/word=1.089 (words=3824969, words/sec=143.89, time=0-17:14:59)   
   Epoch 1.8160: train_loss/word=1.087 (words=3919353, words/sec=147.50, time=0-17:25:39)   
   Epoch 1.8350: train_loss/word=1.086 (words=4010721, words/sec=143.27, time=0-17:36:17)   
   Epoch 1.8540: train_loss/word=1.086 (words=4103969, words/sec=143.31, time=0-17:47:07)   
   Epoch 1.8729: train_loss/word=1.085 (words=4197509, words/sec=141.56, time=0-17:58:08)   
   Epoch 1.8919: train_loss/word=1.084 (words=4289161, words/sec=139.69, time=0-18:09:04)   
   Epoch 1.9109: train_loss/word=1.083 (words=4381085, words/sec=142.35, time=0-18:19:50)   
   Epoch 1.9299: train_loss/word=1.082 (words=4474093, words/sec=141.57, time=0-18:30:47)   
   Epoch 1.9488: train_loss/word=1.081 (words=4567821, words/sec=143.77, time=0-18:41:39)   
   Epoch 1.9678: train_loss/word=1.080 (words=4665777, words/sec=144.71, time=0-18:52:56)   
   Epoch 1.9868: train_loss/word=1.080 (words=4763881, words/sec=140.02, time=0-19:04:36)   
   Epoch 2.0000: train_loss/word=1.079 (words=4825525, words/sec=142.46, time=0-19:11:49)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 2.0000 dev [auxiliary] Loss: 11.866   
     Epoch 2.0000 dev BLEU4: 0.0493742558181, 0.375972/0.102006/0.035359/0.014480 (BP = 0.741710, ratio=0.77, hyp_len=17363, ref_len=22551) (words=90957, words/sec=36.21, time=0-19:53:41)   
     Epoch 2.0000: best dev score, writing model to examples/output/kftt-tree-masktrain.mod   
   Epoch 2.0190: train_loss/word=0.878 (words=92028, words/sec=122.96, time=0-20:06:09)   
   Epoch 2.0380: train_loss/word=0.869 (words=184568, words/sec=139.88, time=0-20:17:11)   
   Epoch 2.0569: train_loss/word=0.866 (words=275864, words/sec=138.87, time=0-20:28:08)   
   Epoch 2.0759: train_loss/word=0.864 (words=369600, words/sec=142.47, time=0-20:39:06)   
   Epoch 2.0949: train_loss/word=0.865 (words=459436, words/sec=140.87, time=0-20:49:44)   
   Epoch 2.1139: train_loss/word=0.868 (words=549524, words/sec=134.91, time=0-21:00:52)   
   Epoch 2.1328: train_loss/word=0.871 (words=644392, words/sec=137.57, time=0-21:12:21)   
   Epoch 2.1518: train_loss/word=0.871 (words=738352, words/sec=144.36, time=0-21:23:12)   
   Epoch 2.1708: train_loss/word=0.874 (words=830432, words/sec=141.86, time=0-21:34:01)   
   Epoch 2.1898: train_loss/word=0.874 (words=923768, words/sec=100.92, time=0-21:49:26)   
   Epoch 2.2087: train_loss/word=0.874 (words=1016572, words/sec=83.40, time=0-22:07:59)   
   Epoch 2.2277: train_loss/word=0.874 (words=1108628, words/sec=79.44, time=0-22:27:17)   
   Epoch 2.2467: train_loss/word=0.875 (words=1198920, words/sec=99.57, time=0-22:42:24)   
   Epoch 2.2657: train_loss/word=0.874 (words=1297252, words/sec=132.70, time=0-22:54:45)   
   Epoch 2.2846: train_loss/word=0.875 (words=1389460, words/sec=124.77, time=0-23:07:04)   
   Epoch 2.3036: train_loss/word=0.876 (words=1480560, words/sec=125.93, time=0-23:19:08)   
   Epoch 2.3226: train_loss/word=0.876 (words=1569956, words/sec=128.12, time=0-23:30:45)   
   Epoch 2.3416: train_loss/word=0.876 (words=1659480, words/sec=131.00, time=0-23:42:09)   
   Epoch 2.3606: train_loss/word=0.876 (words=1748260, words/sec=145.45, time=0-23:52:19)   
   Epoch 2.3795: train_loss/word=0.877 (words=1838916, words/sec=156.25, time=1-00:01:59)   
   Epoch 2.3985: train_loss/word=0.876 (words=1931120, words/sec=162.34, time=1-00:11:27)   
   Epoch 2.4175: train_loss/word=0.877 (words=2021792, words/sec=151.55, time=1-00:21:26)   
   Epoch 2.4365: train_loss/word=0.877 (words=2114204, words/sec=158.14, time=1-00:31:10)   
   Epoch 2.4554: train_loss/word=0.879 (words=2210992, words/sec=149.88, time=1-00:41:56)   
   Epoch 2.4744: train_loss/word=0.879 (words=2301596, words/sec=150.31, time=1-00:51:59)   
   Epoch 2.4934: train_loss/word=0.881 (words=2392080, words/sec=147.67, time=1-01:02:11)   
   Epoch 2.5124: train_loss/word=0.881 (words=2485280, words/sec=151.07, time=1-01:12:28)   
   Epoch 2.5313: train_loss/word=0.882 (words=2576812, words/sec=153.69, time=1-01:22:24)   
   Epoch 2.5503: train_loss/word=0.882 (words=2673144, words/sec=152.82, time=1-01:32:54)   
   Epoch 2.5693: train_loss/word=0.882 (words=2761436, words/sec=160.12, time=1-01:42:06)   
   Epoch 2.5883: train_loss/word=0.883 (words=2851669, words/sec=161.34, time=1-01:51:25)   
   Epoch 2.6073: train_loss/word=0.883 (words=2943289, words/sec=162.21, time=1-02:00:50)   
   Epoch 2.6262: train_loss/word=0.884 (words=3035449, words/sec=109.93, time=1-02:14:48)   
   Epoch 2.6452: train_loss/word=0.884 (words=3125813, words/sec=85.11, time=1-02:32:30)   
   Epoch 2.6642: train_loss/word=0.885 (words=3218677, words/sec=83.65, time=1-02:51:00)   
   Epoch 2.6832: train_loss/word=0.885 (words=3311657, words/sec=81.73, time=1-03:09:58)   
   Epoch 2.7021: train_loss/word=0.886 (words=3407553, words/sec=82.65, time=1-03:29:18)   
   Epoch 2.7211: train_loss/word=0.886 (words=3498873, words/sec=83.44, time=1-03:47:32)   
   Epoch 2.7401: train_loss/word=0.887 (words=3592477, words/sec=83.08, time=1-04:06:19)   
   Epoch 2.7591: train_loss/word=0.887 (words=3684785, words/sec=81.93, time=1-04:25:06)   
   Epoch 2.7781: train_loss/word=0.888 (words=3772025, words/sec=83.13, time=1-04:42:35)   
   Epoch 2.7970: train_loss/word=0.889 (words=3864293, words/sec=81.82, time=1-05:01:23)   
   Epoch 2.8160: train_loss/word=0.889 (words=3959017, words/sec=83.84, time=1-05:20:13)   
   Epoch 2.8350: train_loss/word=0.889 (words=4049281, words/sec=82.52, time=1-05:38:26)   
   Epoch 2.8540: train_loss/word=0.890 (words=4138377, words/sec=81.07, time=1-05:56:45)   
   Epoch 2.8729: train_loss/word=0.890 (words=4224893, words/sec=84.69, time=1-06:13:47)   
   Epoch 2.8919: train_loss/word=0.891 (words=4314705, words/sec=123.67, time=1-06:25:53)   
   Epoch 2.9109: train_loss/word=0.891 (words=4403917, words/sec=158.92, time=1-06:35:15)   
   Epoch 2.9299: train_loss/word=0.891 (words=4492841, words/sec=161.03, time=1-06:44:27)   
   Epoch 2.9488: train_loss/word=0.891 (words=4580349, words/sec=151.22, time=1-06:54:06)   
   Epoch 2.9678: train_loss/word=0.892 (words=4673229, words/sec=141.41, time=1-07:05:02)   
   Epoch 2.9868: train_loss/word=0.892 (words=4764897, words/sec=145.92, time=1-07:15:31)   
   Epoch 3.0000: train_loss/word=0.892 (words=4825525, words/sec=143.28, time=1-07:22:34)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 3.0000 dev [auxiliary] Loss: 11.846   
     Epoch 3.0000 dev BLEU4: 0.0516282931019, 0.391547/0.108439/0.038544/0.015853 (BP = 0.723392, ratio=0.76, hyp_len=17035, ref_len=22551) (words=90957, words/sec=35.71, time=1-08:05:01)   
     Epoch 3.0000: best dev score, writing model to examples/output/kftt-tree-masktrain.mod   
   Epoch 3.0190: train_loss/word=0.686 (words=91236, words/sec=88.53, time=1-08:22:12)   
   Epoch 3.0380: train_loss/word=0.689 (words=180264, words/sec=99.26, time=1-08:37:09)   
   Epoch 3.0569: train_loss/word=0.691 (words=265688, words/sec=102.65, time=1-08:51:01)   
   Epoch 3.0759: train_loss/word=0.694 (words=357492, words/sec=102.87, time=1-09:05:53)   
   Epoch 3.0949: train_loss/word=0.698 (words=449052, words/sec=103.90, time=1-09:20:34)   
   Epoch 3.1139: train_loss/word=0.700 (words=537132, words/sec=108.63, time=1-09:34:05)   
   Epoch 3.1328: train_loss/word=0.702 (words=632332, words/sec=105.57, time=1-09:49:07)   
   Epoch 3.1518: train_loss/word=0.704 (words=725800, words/sec=103.99, time=1-10:04:06)   
   Epoch 3.1708: train_loss/word=0.710 (words=821956, words/sec=99.68, time=1-10:20:10)   
   Epoch 3.1898: train_loss/word=0.711 (words=917056, words/sec=104.65, time=1-10:35:19)   
   Epoch 3.2087: train_loss/word=0.716 (words=1011632, words/sec=102.62, time=1-10:50:41)   
   Epoch 3.2277: train_loss/word=0.714 (words=1104144, words/sec=106.62, time=1-11:05:09)   
   Epoch 3.2467: train_loss/word=0.715 (words=1191524, words/sec=104.91, time=1-11:19:01)   
   Epoch 3.2657: train_loss/word=0.716 (words=1282401, words/sec=106.18, time=1-11:33:17)   
   Epoch 3.2847: train_loss/word=0.717 (words=1373841, words/sec=102.05, time=1-11:48:13)   
   Epoch 3.3036: train_loss/word=0.719 (words=1464781, words/sec=105.45, time=1-12:02:36)   
   Epoch 3.3226: train_loss/word=0.719 (words=1553945, words/sec=103.29, time=1-12:16:59)   
   Epoch 3.3416: train_loss/word=0.721 (words=1647189, words/sec=103.54, time=1-12:32:00)   
   Epoch 3.3606: train_loss/word=0.723 (words=1741209, words/sec=106.94, time=1-12:46:39)   
   Epoch 3.3795: train_loss/word=0.725 (words=1835721, words/sec=102.50, time=1-13:02:01)   
   Epoch 3.3985: train_loss/word=0.725 (words=1925905, words/sec=104.13, time=1-13:16:27)   
   Epoch 3.4175: train_loss/word=0.727 (words=2022489, words/sec=101.53, time=1-13:32:18)   
   Epoch 3.4365: train_loss/word=0.728 (words=2116765, words/sec=101.80, time=1-13:47:44)   
   Epoch 3.4555: train_loss/word=0.729 (words=2207969, words/sec=101.81, time=1-14:02:40)   
   Epoch 3.4744: train_loss/word=0.730 (words=2303025, words/sec=102.28, time=1-14:18:09)   
   Epoch 3.4934: train_loss/word=0.730 (words=2387965, words/sec=99.55, time=1-14:32:23)   
   Epoch 3.5124: train_loss/word=0.731 (words=2477605, words/sec=103.13, time=1-14:46:52)   
   Epoch 3.5314: train_loss/word=0.732 (words=2568585, words/sec=103.18, time=1-15:01:34)   
   Epoch 3.5503: train_loss/word=0.733 (words=2653577, words/sec=100.12, time=1-15:15:43)   
   Epoch 3.5693: train_loss/word=0.734 (words=2747341, words/sec=102.92, time=1-15:30:54)   
   Epoch 3.5883: train_loss/word=0.736 (words=2843521, words/sec=99.60, time=1-15:46:59)   
   Epoch 3.6073: train_loss/word=0.737 (words=2937765, words/sec=101.92, time=1-16:02:24)   
   Epoch 3.6262: train_loss/word=0.737 (words=3028013, words/sec=103.68, time=1-16:16:54)   
   Epoch 3.6452: train_loss/word=0.738 (words=3120117, words/sec=99.16, time=1-16:32:23)   
   Epoch 3.6642: train_loss/word=0.739 (words=3212073, words/sec=102.36, time=1-16:47:22)   
   Epoch 3.6832: train_loss/word=0.740 (words=3302285, words/sec=105.45, time=1-17:01:37)   
   Epoch 3.7021: train_loss/word=0.742 (words=3393557, words/sec=99.87, time=1-17:16:51)   
   Epoch 3.7211: train_loss/word=0.742 (words=3483585, words/sec=101.93, time=1-17:31:34)   
   Epoch 3.7401: train_loss/word=0.744 (words=3576021, words/sec=101.19, time=1-17:46:48)   
   Epoch 3.7591: train_loss/word=0.744 (words=3667857, words/sec=100.49, time=1-18:02:02)   
   Epoch 3.7781: train_loss/word=0.745 (words=3763105, words/sec=97.96, time=1-18:18:14)   
   Epoch 3.7970: train_loss/word=0.746 (words=3852741, words/sec=100.91, time=1-18:33:02)   
   Epoch 3.8160: train_loss/word=0.748 (words=3945441, words/sec=101.80, time=1-18:48:13)   
   Epoch 3.8350: train_loss/word=0.748 (words=4029973, words/sec=101.57, time=1-19:02:05)   
   Epoch 3.8540: train_loss/word=0.749 (words=4120137, words/sec=100.91, time=1-19:16:59)   
   Epoch 3.8729: train_loss/word=0.750 (words=4210397, words/sec=93.93, time=1-19:33:00)   
   Epoch 3.8919: train_loss/word=0.751 (words=4309773, words/sec=99.65, time=1-19:49:37)   
   Epoch 3.9109: train_loss/word=0.752 (words=4402009, words/sec=97.23, time=1-20:05:25)   
   Epoch 3.9299: train_loss/word=0.753 (words=4489597, words/sec=98.91, time=1-20:20:11)   
   Epoch 3.9488: train_loss/word=0.753 (words=4575733, words/sec=107.40, time=1-20:33:33)   
   Epoch 3.9678: train_loss/word=0.755 (words=4667189, words/sec=120.18, time=1-20:46:14)   
   Epoch 3.9868: train_loss/word=0.755 (words=4759789, words/sec=122.15, time=1-20:58:52)   
   Epoch 4.0000: train_loss/word=0.756 (words=4825525, words/sec=118.39, time=1-21:08:07)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
