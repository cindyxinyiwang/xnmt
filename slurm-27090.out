Singularity: Invoking an interactive shell within container...

[0m[dynet] initializing CUDA
Request for 1 GPU ...
[dynet] Device Number: 0
[dynet]   Device name: TITAN X (Pascal)
[dynet]   Memory Clock Rate (KHz): 5005000
[dynet]   Memory Bus Width (bits): 384
[dynet]   Peak Memory Bandwidth (GB/s): 480.48
[dynet]   Memory Free (GB): 12.6169/12.7816
[dynet]
[dynet] Device(s) selected: 0
[dynet] random seed: 3573109590
[dynet] using autobatching
[dynet] allocating memory: 10000MB
[dynet] memory allocation done.
=> Running kftt-tree-bigb-masktrain
   > Preprocessing   
   > Training   
   initialized BilingualTrainingCorpus({'train_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja', 'dev_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.en', 'dev_ref_file': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.upper.en', 'dev_src': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja', 'train_trg': '/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.upparse.en,/projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.en'})   
   initialized PlainTextReader({})   
   initialized TreeReader({})   
   initialized BilingualCorpusParser({'trg_reader': <xnmt.input.TreeReader object at 0x7fc0a8ace8d0>, 'src_reader': <xnmt.input.PlainTextReader object at 0x7fc0a8ace850>})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-train.uppiece.ja   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   initialized SimpleWordEmbedder({'vocab_size': 48841, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>})   
   initialized LSTMSeqTransducer({'layers': 1, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>})   
   initialized StandardAttender({'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>})   
   initialized SimpleWordEmbedder({'vocab_size': 55991, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>})   
   initialized CopyBridge({'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>, 'dec_layers': 1})   
   initialized TreeDecoder({'layers': 1, 'bridge': <xnmt.decoder.CopyBridge object at 0x7fc09396b3d0>, 'vocab_size': 55991, 'mlp_hidden_dim': 512, 'word_lstm': True, 'yaml_context': <xnmt.model_context.ModelContext object at 0x7fc0a8aa7e90>})   
   initialized DefaultTranslator({'attender': <xnmt.attender.StandardAttender object at 0x7fc09396b510>, 'decoder': <xnmt.decoder.TreeDecoder object at 0x7fc09396bd50>, 'src_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7fc0a78be310>, 'loop_trg': True, 'trg_embedder': <xnmt.embedder.SimpleWordEmbedder object at 0x7fc09396b790>, 'encoder': <xnmt.lstm.LSTMSeqTransducer object at 0x7fc0a78be350>})   
   initialized SrcBatcher({'batch_size': 8})   
   Epoch 0.0190: train_loss/word=2.331 (words=109992, words/sec=215.92, time=0-00:08:29)   
   Epoch 0.0380: train_loss/word=2.281 (words=211976, words/sec=210.06, time=0-00:16:34)   
   Epoch 0.0569: train_loss/word=2.209 (words=316656, words/sec=210.05, time=0-00:24:53)   
   Epoch 0.0759: train_loss/word=2.160 (words=420392, words/sec=210.85, time=0-00:33:05)   
   Epoch 0.0949: train_loss/word=2.104 (words=515256, words/sec=219.89, time=0-00:40:16)   
   Epoch 0.1139: train_loss/word=2.067 (words=619744, words/sec=209.75, time=0-00:48:34)   
   Epoch 0.1328: train_loss/word=2.020 (words=716192, words/sec=214.17, time=0-00:56:05)   
   Epoch 0.1518: train_loss/word=1.987 (words=813336, words/sec=204.33, time=0-01:04:00)   
   Epoch 0.1708: train_loss/word=1.955 (words=917168, words/sec=202.56, time=0-01:12:33)   
   Epoch 0.1898: train_loss/word=1.924 (words=1012496, words/sec=207.49, time=0-01:20:12)   
   Epoch 0.2087: train_loss/word=1.896 (words=1114944, words/sec=202.76, time=0-01:28:37)   
   Epoch 0.2277: train_loss/word=1.869 (words=1217184, words/sec=202.80, time=0-01:37:02)   
   Epoch 0.2467: train_loss/word=1.845 (words=1315480, words/sec=199.98, time=0-01:45:13)   
   Epoch 0.2657: train_loss/word=1.819 (words=1423288, words/sec=206.09, time=0-01:53:56)   
   Epoch 0.2846: train_loss/word=1.793 (words=1525016, words/sec=212.23, time=0-02:01:55)   
   Epoch 0.3036: train_loss/word=1.772 (words=1628256, words/sec=205.78, time=0-02:10:17)   
   Epoch 0.3226: train_loss/word=1.751 (words=1728016, words/sec=214.57, time=0-02:18:02)   
   Epoch 0.3416: train_loss/word=1.733 (words=1826232, words/sec=211.65, time=0-02:25:46)   
   Epoch 0.3606: train_loss/word=1.714 (words=1932152, words/sec=211.39, time=0-02:34:07)   
   Epoch 0.3795: train_loss/word=1.696 (words=2036200, words/sec=211.77, time=0-02:42:19)   
   Epoch 0.3985: train_loss/word=1.682 (words=2132920, words/sec=202.47, time=0-02:50:16)   
   Epoch 0.4175: train_loss/word=1.667 (words=2236176, words/sec=206.44, time=0-02:58:36)   
   Epoch 0.4365: train_loss/word=1.652 (words=2337848, words/sec=181.80, time=0-03:07:56)   
   Epoch 0.4554: train_loss/word=1.639 (words=2438200, words/sec=144.01, time=0-03:19:33)   
   Epoch 0.4744: train_loss/word=1.625 (words=2546256, words/sec=129.30, time=0-03:33:28)   
   Epoch 0.4934: train_loss/word=1.612 (words=2646584, words/sec=193.74, time=0-03:42:06)   
   Epoch 0.5124: train_loss/word=1.600 (words=2752288, words/sec=209.87, time=0-03:50:30)   
   Epoch 0.5313: train_loss/word=1.588 (words=2858216, words/sec=207.31, time=0-03:59:01)   
   Epoch 0.5503: train_loss/word=1.576 (words=2965616, words/sec=210.69, time=0-04:07:30)   
   Epoch 0.5693: train_loss/word=1.564 (words=3065368, words/sec=219.95, time=0-04:15:04)   
   Epoch 0.5883: train_loss/word=1.552 (words=3169176, words/sec=216.24, time=0-04:23:04)   
   Epoch 0.6072: train_loss/word=1.542 (words=3273616, words/sec=210.06, time=0-04:31:21)   
   Epoch 0.6262: train_loss/word=1.533 (words=3371256, words/sec=208.37, time=0-04:39:10)   
   Epoch 0.6452: train_loss/word=1.523 (words=3474952, words/sec=214.50, time=0-04:47:13)   
   Epoch 0.6642: train_loss/word=1.514 (words=3578040, words/sec=208.64, time=0-04:55:27)   
   Epoch 0.6832: train_loss/word=1.507 (words=3680536, words/sec=190.68, time=0-05:04:25)   
   Epoch 0.7021: train_loss/word=1.499 (words=3783368, words/sec=198.76, time=0-05:13:02)   
   Epoch 0.7211: train_loss/word=1.491 (words=3886824, words/sec=215.50, time=0-05:21:02)   
   Epoch 0.7401: train_loss/word=1.482 (words=3986920, words/sec=220.84, time=0-05:28:36)   
   Epoch 0.7591: train_loss/word=1.474 (words=4088872, words/sec=201.56, time=0-05:37:01)   
   Epoch 0.7780: train_loss/word=1.467 (words=4187152, words/sec=192.28, time=0-05:45:33)   
   Epoch 0.7970: train_loss/word=1.459 (words=4300512, words/sec=175.96, time=0-05:56:17)   
   Epoch 0.8160: train_loss/word=1.452 (words=4394952, words/sec=175.51, time=0-06:05:15)   
   Epoch 0.8350: train_loss/word=1.445 (words=4498520, words/sec=171.16, time=0-06:15:20)   
   Epoch 0.8539: train_loss/word=1.438 (words=4601456, words/sec=178.53, time=0-06:24:57)   
   Epoch 0.8729: train_loss/word=1.431 (words=4709113, words/sec=175.31, time=0-06:35:11)   
   Epoch 0.8919: train_loss/word=1.425 (words=4813601, words/sec=176.31, time=0-06:45:03)   
   Epoch 0.9109: train_loss/word=1.418 (words=4920217, words/sec=181.84, time=0-06:54:50)   
   Epoch 0.9299: train_loss/word=1.412 (words=5026769, words/sec=179.39, time=0-07:04:44)   
   Epoch 0.9488: train_loss/word=1.406 (words=5131401, words/sec=175.82, time=0-07:14:39)   
   Epoch 0.9678: train_loss/word=1.400 (words=5228737, words/sec=178.33, time=0-07:23:44)   
   Epoch 0.9868: train_loss/word=1.394 (words=5330697, words/sec=177.43, time=0-07:33:19)   
   Epoch 1.0000: train_loss/word=1.390 (words=5403737, words/sec=180.51, time=0-07:40:04)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 1.0000 dev [auxiliary] Loss: 10.753   
     Epoch 1.0000 dev BLEU4: 0.0273814240083, 0.343787/0.077401/0.020828/0.007260 (BP = 0.611365, ratio=0.67, hyp_len=15114, ref_len=22551) (words=101684, words/sec=50.29, time=0-08:13:46)   
     Epoch 1.0000: best dev score, writing model to examples/output/kftt-tree-bigb-masktrain.mod   
   Epoch 1.0190: train_loss/word=0.983 (words=105656, words/sec=159.83, time=0-08:24:47)   
   Epoch 1.0380: train_loss/word=0.991 (words=204712, words/sec=187.28, time=0-08:33:36)   
   Epoch 1.0569: train_loss/word=1.001 (words=304864, words/sec=183.55, time=0-08:42:41)   
   Epoch 1.0759: train_loss/word=0.999 (words=407696, words/sec=186.00, time=0-08:51:54)   
   Epoch 1.0949: train_loss/word=0.997 (words=513208, words/sec=186.54, time=0-09:01:20)   
   Epoch 1.1139: train_loss/word=0.995 (words=614304, words/sec=184.89, time=0-09:10:27)   
   Epoch 1.1328: train_loss/word=0.994 (words=724160, words/sec=179.82, time=0-09:20:37)   
   Epoch 1.1518: train_loss/word=0.989 (words=827481, words/sec=186.44, time=0-09:29:52)   
   Epoch 1.1708: train_loss/word=0.986 (words=920969, words/sec=185.64, time=0-09:38:15)   
   Epoch 1.1898: train_loss/word=0.983 (words=1027185, words/sec=183.57, time=0-09:47:54)   
   Epoch 1.2088: train_loss/word=0.986 (words=1129881, words/sec=176.79, time=0-09:57:35)   
   Epoch 1.2277: train_loss/word=0.989 (words=1231329, words/sec=174.79, time=0-10:07:15)   
   Epoch 1.2467: train_loss/word=0.990 (words=1333881, words/sec=177.19, time=0-10:16:54)   
   Epoch 1.2657: train_loss/word=0.987 (words=1437617, words/sec=186.90, time=0-10:26:09)   
   Epoch 1.2847: train_loss/word=0.988 (words=1544721, words/sec=176.88, time=0-10:36:15)   
   Epoch 1.3036: train_loss/word=0.990 (words=1648697, words/sec=171.63, time=0-10:46:20)   
   Epoch 1.3226: train_loss/word=0.991 (words=1756921, words/sec=173.66, time=0-10:56:44)   
   Epoch 1.3416: train_loss/word=0.991 (words=1861041, words/sec=176.36, time=0-11:06:34)   
   Epoch 1.3606: train_loss/word=0.990 (words=1963249, words/sec=180.92, time=0-11:15:59)   
   Epoch 1.3795: train_loss/word=0.989 (words=2063609, words/sec=181.35, time=0-11:25:12)   
   Epoch 1.3985: train_loss/word=0.989 (words=2168001, words/sec=178.97, time=0-11:34:56)   
   Epoch 1.4175: train_loss/word=0.989 (words=2274449, words/sec=178.04, time=0-11:44:53)   
   Epoch 1.4365: train_loss/word=0.988 (words=2377153, words/sec=180.17, time=0-11:54:23)   
   Epoch 1.4555: train_loss/word=0.988 (words=2478593, words/sec=179.12, time=0-12:03:50)   
   Epoch 1.4744: train_loss/word=0.985 (words=2577553, words/sec=187.01, time=0-12:12:39)   
   Epoch 1.4934: train_loss/word=0.985 (words=2681393, words/sec=177.27, time=0-12:22:25)   
   Epoch 1.5124: train_loss/word=0.984 (words=2785489, words/sec=179.56, time=0-12:32:04)   
   Epoch 1.5314: train_loss/word=0.985 (words=2898505, words/sec=175.24, time=0-12:42:49)   
   Epoch 1.5503: train_loss/word=0.983 (words=2993129, words/sec=180.73, time=0-12:51:33)   
   Epoch 1.5693: train_loss/word=0.981 (words=3096529, words/sec=180.61, time=0-13:01:05)   
   Epoch 1.5883: train_loss/word=0.980 (words=3199273, words/sec=177.36, time=0-13:10:45)   
   Epoch 1.6073: train_loss/word=0.980 (words=3301945, words/sec=170.91, time=0-13:20:46)   
   Epoch 1.6262: train_loss/word=0.979 (words=3402681, words/sec=182.10, time=0-13:29:59)   
   Epoch 1.6452: train_loss/word=0.978 (words=3500129, words/sec=183.34, time=0-13:38:50)   
   Epoch 1.6642: train_loss/word=0.976 (words=3600577, words/sec=181.43, time=0-13:48:04)   
   Epoch 1.6832: train_loss/word=0.975 (words=3707361, words/sec=180.29, time=0-13:57:56)   
   Epoch 1.7021: train_loss/word=0.974 (words=3807329, words/sec=179.20, time=0-14:07:14)   
   Epoch 1.7211: train_loss/word=0.972 (words=3910049, words/sec=184.50, time=0-14:16:31)   
   Epoch 1.7401: train_loss/word=0.970 (words=4019857, words/sec=185.03, time=0-14:26:24)   
   Epoch 1.7591: train_loss/word=0.969 (words=4128721, words/sec=175.88, time=0-14:36:43)   
   Epoch 1.7781: train_loss/word=0.968 (words=4229985, words/sec=179.00, time=0-14:46:09)   
   Epoch 1.7970: train_loss/word=0.967 (words=4332633, words/sec=185.28, time=0-14:55:23)   
   Epoch 1.8160: train_loss/word=0.966 (words=4434505, words/sec=180.14, time=0-15:04:48)   
   Epoch 1.8350: train_loss/word=0.965 (words=4531001, words/sec=178.35, time=0-15:13:50)   
   Epoch 1.8540: train_loss/word=0.963 (words=4628641, words/sec=178.49, time=0-15:22:57)   
   Epoch 1.8729: train_loss/word=0.963 (words=4727961, words/sec=178.08, time=0-15:32:14)   
   Epoch 1.8919: train_loss/word=0.963 (words=4831577, words/sec=179.23, time=0-15:41:52)   
   Epoch 1.9109: train_loss/word=0.962 (words=4930337, words/sec=181.35, time=0-15:50:57)   
   Epoch 1.9299: train_loss/word=0.960 (words=5026865, words/sec=181.77, time=0-15:59:48)   
   Epoch 1.9488: train_loss/word=0.959 (words=5122345, words/sec=185.94, time=0-16:08:22)   
   Epoch 1.9678: train_loss/word=0.959 (words=5229673, words/sec=181.79, time=0-16:18:12)   
   Epoch 1.9868: train_loss/word=0.958 (words=5333889, words/sec=175.49, time=0-16:28:06)   
   Epoch 2.0000: train_loss/word=0.958 (words=5403737, words/sec=180.29, time=0-16:34:33)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 2.0000 dev [auxiliary] Loss: 10.633   
     Epoch 2.0000 dev BLEU4: 0.0548255521438, 0.316595/0.083979/0.028309/0.012021 (BP = 0.999645, ratio=1.00, hyp_len=22543, ref_len=22551) (words=101684, words/sec=36.30, time=0-17:21:15)   
     Epoch 2.0000: best dev score, writing model to examples/output/kftt-tree-bigb-masktrain.mod   
   Epoch 2.0190: train_loss/word=0.769 (words=107896, words/sec=151.98, time=0-17:33:05)   
   Epoch 2.0380: train_loss/word=0.765 (words=203528, words/sec=182.88, time=0-17:41:48)   
   Epoch 2.0569: train_loss/word=0.762 (words=297984, words/sec=180.74, time=0-17:50:30)   
   Epoch 2.0759: train_loss/word=0.771 (words=406280, words/sec=180.40, time=0-18:00:30)   
   Epoch 2.0949: train_loss/word=0.775 (words=512480, words/sec=176.69, time=0-18:10:31)   
   Epoch 2.1139: train_loss/word=0.775 (words=609920, words/sec=182.72, time=0-18:19:25)   
   Epoch 2.1328: train_loss/word=0.772 (words=707032, words/sec=183.33, time=0-18:28:14)   
   Epoch 2.1518: train_loss/word=0.773 (words=806744, words/sec=183.62, time=0-18:37:17)   
   Epoch 2.1708: train_loss/word=0.775 (words=910488, words/sec=178.56, time=0-18:46:58)   
   Epoch 2.1898: train_loss/word=0.773 (words=1015144, words/sec=182.93, time=0-18:56:31)   
   Epoch 2.2087: train_loss/word=0.774 (words=1118440, words/sec=181.07, time=0-19:06:01)   
   Epoch 2.2277: train_loss/word=0.772 (words=1218432, words/sec=185.18, time=0-19:15:01)   
   Epoch 2.2467: train_loss/word=0.773 (words=1324528, words/sec=178.56, time=0-19:24:55)   
   Epoch 2.2657: train_loss/word=0.775 (words=1436560, words/sec=178.66, time=0-19:35:22)   
   Epoch 2.2846: train_loss/word=0.775 (words=1537984, words/sec=182.53, time=0-19:44:38)   
   Epoch 2.3036: train_loss/word=0.775 (words=1639600, words/sec=178.92, time=0-19:54:06)   
   Epoch 2.3226: train_loss/word=0.778 (words=1750248, words/sec=174.07, time=0-20:04:42)   
   Epoch 2.3416: train_loss/word=0.779 (words=1851784, words/sec=179.35, time=0-20:14:08)   
   Epoch 2.3606: train_loss/word=0.779 (words=1953520, words/sec=182.51, time=0-20:23:25)   
   Epoch 2.3795: train_loss/word=0.778 (words=2046744, words/sec=183.80, time=0-20:31:52)   
   Epoch 2.3985: train_loss/word=0.778 (words=2144592, words/sec=180.88, time=0-20:40:53)   
   Epoch 2.4175: train_loss/word=0.777 (words=2241200, words/sec=184.75, time=0-20:49:36)   
   Epoch 2.4365: train_loss/word=0.777 (words=2337712, words/sec=180.77, time=0-20:58:30)   
   Epoch 2.4554: train_loss/word=0.778 (words=2443616, words/sec=181.22, time=0-21:08:14)   
   Epoch 2.4744: train_loss/word=0.778 (words=2542552, words/sec=178.27, time=0-21:17:29)   
   Epoch 2.4934: train_loss/word=0.778 (words=2641288, words/sec=186.30, time=0-21:26:19)   
   Epoch 2.5124: train_loss/word=0.778 (words=2743240, words/sec=185.65, time=0-21:35:29)   
   Epoch 2.5313: train_loss/word=0.777 (words=2848248, words/sec=190.96, time=0-21:44:38)   
   Epoch 2.5503: train_loss/word=0.777 (words=2950888, words/sec=179.68, time=0-21:54:10)   
   Epoch 2.5693: train_loss/word=0.778 (words=3057032, words/sec=176.52, time=0-22:04:11)   
   Epoch 2.5883: train_loss/word=0.779 (words=3164032, words/sec=176.13, time=0-22:14:18)   
   Epoch 2.6072: train_loss/word=0.779 (words=3272368, words/sec=183.88, time=0-22:24:08)   
   Epoch 2.6262: train_loss/word=0.779 (words=3372512, words/sec=184.00, time=0-22:33:12)   
   Epoch 2.6452: train_loss/word=0.778 (words=3477016, words/sec=185.44, time=0-22:42:35)   
   Epoch 2.6642: train_loss/word=0.779 (words=3573392, words/sec=179.70, time=0-22:51:32)   
   Epoch 2.6832: train_loss/word=0.778 (words=3674152, words/sec=181.51, time=0-23:00:47)   
   Epoch 2.7021: train_loss/word=0.779 (words=3782272, words/sec=181.49, time=0-23:10:43)   
   Epoch 2.7211: train_loss/word=0.778 (words=3887864, words/sec=180.37, time=0-23:20:28)   
   Epoch 2.7401: train_loss/word=0.778 (words=3991704, words/sec=181.51, time=0-23:30:00)   
   Epoch 2.7591: train_loss/word=0.778 (words=4102464, words/sec=183.57, time=0-23:40:04)   
   Epoch 2.7780: train_loss/word=0.779 (words=4209184, words/sec=178.55, time=0-23:50:01)   
   Epoch 2.7970: train_loss/word=0.779 (words=4316600, words/sec=180.65, time=0-23:59:56)   
   Epoch 2.8160: train_loss/word=0.779 (words=4415720, words/sec=188.43, time=1-00:08:42)   
   Epoch 2.8350: train_loss/word=0.779 (words=4522808, words/sec=180.76, time=1-00:18:34)   
   Epoch 2.8539: train_loss/word=0.779 (words=4626464, words/sec=182.11, time=1-00:28:03)   
   Epoch 2.8729: train_loss/word=0.778 (words=4725544, words/sec=191.68, time=1-00:36:40)   
   Epoch 2.8919: train_loss/word=0.778 (words=4829160, words/sec=184.88, time=1-00:46:01)   
   Epoch 2.9109: train_loss/word=0.778 (words=4930632, words/sec=179.22, time=1-00:55:27)   
   Epoch 2.9298: train_loss/word=0.778 (words=5032744, words/sec=183.35, time=1-01:04:44)   
   Epoch 2.9488: train_loss/word=0.778 (words=5138633, words/sec=187.89, time=1-01:14:08)   
   Epoch 2.9678: train_loss/word=0.778 (words=5234689, words/sec=183.07, time=1-01:22:52)   
   Epoch 2.9868: train_loss/word=0.778 (words=5335281, words/sec=182.40, time=1-01:32:04)   
   Epoch 3.0000: train_loss/word=0.778 (words=5403737, words/sec=182.64, time=1-01:38:19)   
   initialized PolynomialNormalization({'apply_during_search': True})   
   /projects/tir2/users/xinyiw1/kftt-data-1.0/data/small/kyoto-tune.uppiece.ja   
   > Checkpoint   
     Epoch 3.0000 dev [auxiliary] Loss: 10.603   
     Epoch 3.0000 dev BLEU4: 0.0530701112854, 0.411441/0.118031/0.042545/0.019901 (BP = 0.662736, ratio=0.71, hyp_len=15978, ref_len=22551) (words=101684, words/sec=48.75, time=1-02:13:04)   
   Epoch 3.0190: train_loss/word=0.580 (words=99264, words/sec=186.97, time=1-02:21:55)   
   Epoch 3.0380: train_loss/word=0.602 (words=201192, words/sec=185.06, time=1-02:31:06)   
   Epoch 3.0569: train_loss/word=0.612 (words=299448, words/sec=176.39, time=1-02:40:23)   
   Epoch 3.0759: train_loss/word=0.617 (words=403024, words/sec=182.76, time=1-02:49:50)   
   Epoch 3.0949: train_loss/word=0.616 (words=503840, words/sec=182.59, time=1-02:59:02)   
   Epoch 3.1139: train_loss/word=0.614 (words=603136, words/sec=191.61, time=1-03:07:40)   
   Epoch 3.1328: train_loss/word=0.614 (words=709728, words/sec=188.20, time=1-03:17:06)   
   Epoch 3.1518: train_loss/word=0.616 (words=809392, words/sec=180.94, time=1-03:26:17)   
   Epoch 3.1708: train_loss/word=0.620 (words=912496, words/sec=178.51, time=1-03:35:55)   
   Epoch 3.1898: train_loss/word=0.619 (words=1008128, words/sec=189.14, time=1-03:44:20)   
   Epoch 3.2087: train_loss/word=0.619 (words=1110160, words/sec=186.48, time=1-03:53:27)   
   Epoch 3.2277: train_loss/word=0.619 (words=1211712, words/sec=182.97, time=1-04:02:42)   
   Epoch 3.2467: train_loss/word=0.619 (words=1307816, words/sec=185.89, time=1-04:11:19)   
   Epoch 3.2657: train_loss/word=0.622 (words=1417216, words/sec=182.82, time=1-04:21:18)   
   Epoch 3.2846: train_loss/word=0.621 (words=1518488, words/sec=186.97, time=1-04:30:19)   
   Epoch 3.3036: train_loss/word=0.621 (words=1616216, words/sec=187.16, time=1-04:39:02)   
   Epoch 3.3226: train_loss/word=0.623 (words=1723168, words/sec=181.89, time=1-04:48:50)   
   Epoch 3.3416: train_loss/word=0.624 (words=1822336, words/sec=187.07, time=1-04:57:40)   
   Epoch 3.3606: train_loss/word=0.623 (words=1922152, words/sec=188.22, time=1-05:06:30)   
   Epoch 3.3795: train_loss/word=0.623 (words=2028752, words/sec=186.85, time=1-05:16:01)   
   Epoch 3.3985: train_loss/word=0.625 (words=2127432, words/sec=182.15, time=1-05:25:02)   
   Epoch 3.4175: train_loss/word=0.626 (words=2234960, words/sec=180.87, time=1-05:34:57)   
